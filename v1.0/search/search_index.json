{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to ExaMon Documentation</p>"},{"location":"About/","title":"About","text":"<p>ExaMon is an open source framework developed by Francesco Beneventi at DEI - Department of Electrical, Electronic, and Information Engineering \"Guglielmo Marconi\" of the University of Bologna under the supervision of Profs. Luca Benini, Andrea Bartolini and Andrea Borghesi and in collaboration with CINECA and E4.</p>"},{"location":"Introduction/","title":"Introduction","text":"<p>ExaMon (Exascale Monitoring) is a data collection and analysis platform designed to manage large amounts of data. Its main prerogatives are to easily manage heterogeneous data, both in streaming and batch mode and to provide access to this data through a common interface. This simplifies the use of data to support applications such as real-time anomaly detection, predictive maintenance, and efficient resource and energy management leveraging machine learning and artificial intelligence techniques. Due to its scalable and distributed nature, it is easily applicable to HPC systems, especially exascale-sized ones, the primary use case for which it was designed.</p> <p>The key feature of the framework is its data model, designed to be schema-less and scalable. In this way, it allows to collect a huge amount of heterogeneous data under a single interface. This data lake, which makes all the data available online to any user at any time, is proposed as a solution to break down internal data silos in organizations. The main benefit of this approach is that it enables capturing the full value of the data by making it immediately usable. In addition, having all the data in one place makes it easier to create complete and immediate executive reports, enabling faster and more informed decisions.</p> <p>Another key aspect of the framework's design is making industry data easily available for research purposes. Indeed, researchers only need to manage a single data source to have a complete picture of complex industrial systems, and the benefits can be many. The ease of access to a huge variety and quantity of real-world data will enable them to create innovative solutions with results that may have real-world impact.</p> <p></p> <p>Furthermore, access to a wide variety of heterogeneous data with very low latency enables the realization of accurate digital twins. In this regard, the framework can provide both historical data for building accurate models, and fresh data for quickly making inferences on the same models. Moreover, the availability of up-to-date data in near real-time allows the construction of visual models that enable the rapid acquisition of knowledge about the state of any complex system. In fact, by exploiting the language of visual communication, it is possible to extend collaboration by bringing together a wide range of experts focused on problem-solving or optimization of the system itself.</p> <p></p> <p>The architecture of the framework is based on established protocols and technologies rather than specific tools and implementations. The communication layer is based on the publish-subscribe model that finds various implementations, such as in the MQTT protocol. The need to interact with different data sources, ranging from complex room cooling systems to internal CPU sensors, requires a simple, scalable, low-latency communication protocol that is resilient to network conditions and natively designed to enable machine-to-machine (M2M) communication in complex environments. Moreover, data persistence is handled by a NoSQL-type database, an industry-proven technology, designed to be horizontally scalable and built to efficiently handle large amounts of data. On top of these two pillars, the other components are primarily dedicated to handling the two main categories of data that characterize the ExaMon framework. The first is the time series data type, which represents the majority of the data sources managed by ExaMon and is suitable for managing all the sensors and logs available today in a data center. The second is the generic tabular data type, suitable for managing metadata and any other data that does not fall into the first category. ExaMon provides the tools and interfaces to coordinate these two categories and interface them with the user in the most seamless way.</p> <p>As a data platform, one of ExaMon's priorities is data sharing. To maximize its effectiveness, it offers both domain-specific interfaces (DSLs), which allow more experienced users to take full advantage of the data source's capabilities, and more high-level, standard interfaces such as the ANSI SQL language. Again, ExaMon promotes tools that are state of the art for time series data visualization, such as Grafana. Although more experienced users can interface with ExaMon using tools such as Jupyter notebooks (via a dedicated client), more user-friendly BI solutions such as Apache Superset, which uses web visualization technologies and the ANSI SQL language, are also provided to streamline the user experience. There is also compatibility with tools such as Apache Spark and Dask for large-scale data analysis in both streaming and batch modes. Finally, CLI-type tools are also available to provide access to the data and typical features directly from the user's shell.</p>"},{"location":"Introduction_copy/","title":"Introduction copy","text":"<p>Warning</p> <p>This is an only an early preview and the text may be obsolete.</p>"},{"location":"Introduction_copy/#examon","title":"ExaMon","text":""},{"location":"Introduction_copy/#introduction","title":"Introduction","text":"<p>ExaMon, short for Exascale Monitoring, is a powerful data collection and analytics platform specifically designed for managing big data. It is primarily developed and utilized for monitoring High-Performance Computing (HPC) systems, particularly those operating at exascale levels. ExaMon simplifies the real-time streaming and batch processing of heterogeneous data, providing a unified interface for accessing and analyzing this data. With its advanced capabilities in machine learning and artificial intelligence, ExaMon enables real-time anomaly detection, predictive maintenance, and efficient resource and energy management. Its versatile architecture and capabilities make it applicable to a wide range of use cases beyond HPC systems.</p>"},{"location":"Introduction_copy/#architecture","title":"Architecture","text":"<p>ExaMon is a modular system comprised of multiple layers, each equipped with its own set of components. This architectural design enables seamless integration of diverse data sources, provided they conform to the specified data formats. At the core of ExaMon lies the middleware layer, which utilizes MQTT (Message Queuing Telemetry Transport) brokers to receive data generated by low-level plugins. The received data is then uniformly formatted and stored in the data storage layer. To simplify access and utilization of the collected and stored data, ExaMon provides a client interface that enables high-level applications to interact with the underlying data. This client interface serves as a gateway for accessing and harnessing the valuable information within ExaMon.</p> <p></p>"},{"location":"Introduction_copy/#sensor-collectors","title":"Sensor Collectors","text":"<p>The collector layer comprises low-level components responsible for reading data from various sensors scattered throughout the system and delivering it in a standardized format to the upper layers of the stack. These software components consist of two main objects: the MQTT API and the Sensor API object. The MQTT API implements the MQTT protocol functions, which are consistent across all collectors. The Sensor API object, on the other hand, implements custom sensor functions related to data sampling and is unique to each collector. The specific sensor API object can be classified as either having direct access to hardware resources (such as IPMI or PMU units in a CPU) or sampling data from other monitoring applications (such as Ganglia and Nagios) and batch schedulers (such as Slurm).</p>"},{"location":"Introduction_copy/#communication-layer","title":"Communication layer","text":"<p>The ExaMon framework utilizes the MQTT protocol, which implements the \"publish-subscribe\" messaging pattern and requires three agents to function: the publisher, subscriber, and broker. The publisher sends data on a specific topic, while the subscriber subscribes to the appropriate topic to receive the required data. The broker has the responsibility of receiving data from publishers, making topics available to subscribers, and delivering data to subscribers. In the MQTT communication mechanism, when a publisher sends data with a specific topic as a protocol parameter, the topic is created and made available to the broker. Any subscriber to that topic will receive the associated data as soon as it becomes available to the broker. In this context, collector agents act as publishers by sending data to the broker.</p>"},{"location":"Introduction_copy/#storage-layer","title":"Storage layer","text":"<p>ExaMon provides a mechanism to store metrics for visualization and analysis of historical data using a distributed and scalable time series database called KairosDB, built on top of the NoSQL database Apache Cassandra. To enable the insertion of data into KairosDB, ExaMon implements a specific MQTT subscriber called MQTT2Kairos, which acts as a bridge between the MQTT protocol and the KairosDB insertion mechanism. This bridge takes advantage of the MQTT topics structure of the monitoring framework to automatically form the KairosDB insertion statement, resulting in several advantages. First, it reduces the computational overhead of the bridge to a string parsing operation per message. Second, it simplifies the formation of database queries, starting only from the knowledge of the matching MQTT topic. Lastly, it decouples the transport layer from the storage layer, making it easy to migrate to new data storage systems.</p>"},{"location":"Introduction_copy/#applications-layer","title":"Applications Layer","text":"<p>The application layer of the ExaMon framework enables users to extract insights and intelligence from the data collected by the monitoring system. The collected data can be visualized using web-based tools or used for machine learning applications such as predictive modeling and online fault detection. The ExaMon framework provides a convenient software component called the examon-client, which serves as a consistent and unified interface between the heterogeneous storage layer and the applications. With this component, users can easily access the data collected by the system and use it for their specific applications without needing to worry about the underlying data formats or storage mechanisms. The examon-client also provides various APIs and libraries to facilitate the development of new applications that use the data collected by ExaMon.</p>"},{"location":"Introduction_copy/#data-model","title":"Data model","text":"<p>ExaMon's data model is designed to handle large and diverse data sets, and it adopts a non-relational, hierarchical data model to achieve this. At the core of this model is the concept of a \"metric\", which represents the physical or abstract entity generating data. A metric generates a value, which could be a number, a string, or a line of text, and is associated with a timestamp indicating the time of collection. Additionally, each metric can have one or more tags in the form of key-value pairs, which provide detailed information about the metric's properties. This hierarchical structure allows for efficient storage and retrieval of data, as well as flexible querying and analysis.</p> <p>In ExaMon, the tags are used to define extra information about the sensor and some special mandatory properties of the protocol.</p> <p></p> <p>The data model resembles a hierarchical tree. In the Figure are shown the relevant sections.</p> <ul> <li>Sensor location: (Mandatory) It is a free hierarchical sequence of key/value couples used to locate the data source. At least one couple should be present.</li> <li>Plugin name: (Mandatory) It is the name of the data collector agent (plugin) which acquires data from this sensor</li> <li>Channel type: (Mandatory) specifies the type of channel<ul> <li>data: metrics values sent by the plugin</li> <li>cmd: commands sent to the sensor/plugin</li> </ul> </li> <li>Specific plugin tags: (Optional) additional free key/values couples to add custom plugin attributes that are unique for this data source.</li> </ul> <p>The data source or a sensor is effectively defined when it is possible to uniquely determine a complete path within the tag hierarchical tree, from top to bottom. This means that in ExaMon, a sensor can only have one value for each of its tags.</p> <p>Currently, to enter data within ExaMon you must connect to the transport layer (MQTT Broker) and send messages in a specific format.</p>"},{"location":"Introduction_copy/#transport","title":"Transport","text":"<p>The ExaMon data model is well-suited for use with the MQTT protocol in the transport layer.</p> <p>In MQTT, the tags and metric name define the topic, while the value and timestamp are part of the payload. This mapping fits well with the ExaMon data model, where each metric is associated with specific tags that define its context.</p> <p>By leveraging the MQTT protocol, ExaMon can efficiently transport data across distributed systems, allowing for real-time monitoring and analysis of system performance. MQTT provides a lightweight and efficient communication protocol that is well-suited for IoT and other resource-constrained systems.</p> <p></p>"},{"location":"Introduction_copy/#mqtt-topic","title":"MQTT Topic","text":"<p>The following figure shows in detail the structure of an MQTT topic implementing the ExaMon data model.</p> <p></p>"},{"location":"Introduction_copy/#mqtt-payload","title":"MQTT Payload","text":"<p>The MQTT payload, in the basic implementation of ExaMon, is a string obtained by concatenating the measured value and the timestamp associated with it.</p> <p></p>"},{"location":"Introduction_copy/#example","title":"Example","text":"<p>An example of the MQTT data that are compatible with the ExaMon data model is the following (Obtained from the reference implementation in pmu_pub):</p> Topic: org/testorg/cluster/testcluster/node/testnode00/plugin/pmu_pub/chnl/data/core/23/aperf Payload: 10822010350093;1658832078.001"},{"location":"Introduction_copy/#storage","title":"Storage","text":"<p>In the storage layer of ExaMon, the Cassandra table schema maps the data model. Cassandra tables are collections of rows, where each row has a primary key. KairosDB is used by ExaMon to define the primary keys in Cassandra and enable efficient indexing and searching of data. The primary keys are a concatenation of the metric name and tags, and they are used for data indexing and searching. The values and timestamps represent the columns of the row.</p> <p></p>"},{"location":"Introduction_copy/#query-language","title":"Query language","text":"<p>The data stored by ExaMon can be retrieved by querying the Cassandra and KairosDB modules directly. However, since the data is stored in a de-normalized form in Cassandra, managing the data from an application perspective can be complex, despite the benefits of improved scalability and performance.</p> <p>To simplify the development of applications that use ExaMon data, a uniform entry point for all data was developed and named Examon-client. This uniform interface provides a simplified and consistent view of the data, hiding the complexities of data management from the application developer.</p> <p>Examon-client allows users to access monitoring and analytics data from ExaMon using a common set of queries and provides a simpler and more intuitive way to interact with the data. In the following section, we will describe the features and capabilities of Examon-client in more detail.</p>"},{"location":"Introduction_copy/#examon-client","title":"Examon-client","text":"<p>Examon-client is a Python package that provides a uniform interface for accessing and analyzing data stored in the ExaMon database. This client enables querying of metrics from the database through a pluggable interface. In the current implementation, the client uses the KairosDB REST API to retrieve data.</p> <p>Examon-client provides a SQL-like query language for ease of use, allowing users to retrieve data simply and intuitively. The client is designed to work seamlessly with ExaMon's storage architecture, currently based on Cassandra and KairosDB.</p> <p>With Examon-client, ExaMon data can be accessed locally using the Pandas interface or in a distributed fashion using Apache Spark or Dask. This makes it possible to handle heavy workloads efficiently and effectively.</p> <p></p> <p>Queries can be built and executed with an SQL-like language.</p> <pre><code>data = sq.SELECT('*') \\\n      .FROM('p0_power') \\\n      .WHERE(cluster='marconi100' node='r255n18') \\\n      .TSTART(10,'minutes') \\\n      .execute()\n</code></pre> <p>The data returned by the query is a dataframe.</p> <p></p>"},{"location":"Introduction_copy/#data-visualization","title":"Data Visualization","text":"<p>In addition to querying data through Examon-client, users can also access ExaMon data through the integrated visualization tool, Grafana. Grafana is a high-level component that provides live visualization of different data sources, including information about completed jobs, system service statuses, and hardware sensors' measurements.</p> <p>A wide range of views and pre-built dashboards can be accessed using the dedicated Grafana server. For example, by selecting specific nodes and a time interval (such as the last 5 minutes), users can visualize different metrics as time series and/or aggregated values (average, max/min, variance, etc.).</p> <p></p> <p>Alternatively, Grafana can be also used to gain an overall view of all the racks of the data centers, with many potential benefits, among them identifying nodes with anomalous thermal load or workload imbalance.</p> <p></p> <p>Finally, it is possible to visualize the usage of the computing resources made by the users through the metrics acquired from the job scheduler (Slurm).</p> <p></p> <p>Grafana provides an intuitive and user-friendly interface for exploring and visualizing ExaMon data, enabling users to quickly and easily gain insights into system performance and behavior. With its extensive range of features and customization options, Grafana is a powerful tool for analyzing and visualizing ExaMon data.</p>"},{"location":"Introduction_prev/","title":"Introduction prev","text":"<p>Warning</p> <p>This is an only an early preview and the text may be obsolete.</p> <p></p>"},{"location":"Introduction_prev/#introduction","title":"Introduction","text":"<p>ExaMon (Exascale Monitoring) is a powerful data collection and analytics platform designed for managing big data. It is primarily developed and used for monitoring HPC systems, especially those at exascale levels. ExaMon simplifies the management of heterogeneous data in real-time streaming and batch modes, providing a single interface for accessing and analyzing this data. With its advanced capabilities in machine learning and artificial intelligence, ExaMon supports real-time anomaly detection, predictive maintenance, and efficient resource and energy management. However, its architecture and capabilities make it a versatile solution that can be applied to other use cases as well.</p>"},{"location":"Introduction_prev/#architecture","title":"Architecture","text":"<p>ExaMon consists of several layers, each with its own set of components. The compositional nature of the infrastructure allows seamless integration of different data sources, provided they match the correct data formats. At the core of ExaMon is the middleware layer, powered by Message Queuing Telemetry Transport (MQTT) brokers, which act as receivers of data generated by low-level plug-ins. The data is then uniformly formatted and stored in the data storage layer. Finally, high-level applications can easily access and utilise the underlying data collected and stored in ExaMon via a client interface.</p> <p></p>"},{"location":"Introduction_prev/#sensor-collectors","title":"Sensor Collectors","text":"<p>The collector layer consists of low-level components that are responsible for reading data from various sensors scattered throughout the system and delivering it in a standardised format to the upper layers of the stack. These software components are made up of two main objects: the Transport API and the Sensor API object. The Transport API implements the MQTT protocol functions that are consistent across all collectors. The Sensor API object, on the other hand, implements custom sensor functions related to data sampling and is unique to each type of collector. The specific Sensor API object can be classified as either having direct access to hardware resources (such as IPMI or PMU units in a CPU) or sampling data from other monitoring applications (such as Ganglia and Nagios) and batch schedulers (such as Slurm).</p>"},{"location":"Introduction_prev/#communication-layer","title":"Communication layer","text":"<p>The ExaMon framework employs the MQTT protocol, which implements the \"publish-subscribe\" messaging pattern and requires three agents to function: the publisher, the subscriber and the broker. The publisher sends data about a specific topic, while the subscriber subscribes to the appropriate topic to receive the required data. The broker is responsible for receiving data from publishers, making topics available to subscribers and delivering data to subscribers. In the MQTT communication mechanism, when a publisher sends data with a specific topic as a protocol parameter, the topic is created and made available to the broker. Any subscriber to that topic will receive the associated data as it becomes available to the broker. In this context, collector agents act as publishers by sending data to the broker.</p>"},{"location":"Introduction_prev/#storage-layer","title":"Storage layer","text":"<p>ExaMon provides a mechanism to store metrics for visualization and analysis of historical data using a distributed and scalable time series database called KairosDB, built on top of the NoSQL database Apache Cassandra. To enable the insertion of data into KairosDB, ExaMon implements a specific MQTT subscriber called MQTT2Kairos, which acts as a bridge between the MQTT protocol and the KairosDB insertion mechanism. This bridge takes advantage of the MQTT topics structure of the monitoring framework to automatically form the KairosDB insertion statement, resulting in several advantages. First, it reduces the computational overhead of the bridge to a string parsing operation per message. Second, it simplifies the formation of database queries, starting only from the knowledge of the matching MQTT topic. Lastly, it decouples the transport layer from the storage layer, making it easy to migrate to new data storage systems.</p>"},{"location":"Introduction_prev/#applications-layer","title":"Applications Layer","text":"<p>The application layer of the ExaMon framework enables users to extract insights and intelligence from the data collected by the monitoring system. The collected data can be visualized using web-based tools or used for machine learning applications such as predictive modeling and online fault detection. The ExaMon framework provides a convenient software component called the examon-client, which serves as a consistent and unified interface between the heterogeneous storage layer and the applications. With this component, users can easily access the data collected by the system and use it for their specific applications without needing to worry about the underlying data formats or storage mechanisms. The examon-client also provides various APIs and libraries to facilitate the development of new applications that use the data collected by ExaMon.</p>"},{"location":"Introduction_prev/#data-model","title":"Data model","text":"<p>ExaMon's data model is designed to handle large and diverse data sets, and it adopts a non-relational, hierarchical data model to achieve this. At the core of this model is the concept of a \"metric\", which represents the physical or abstract entity generating data. A metric generates a value, which could be a number, a string, or a line of text, and is associated with a timestamp indicating the time of collection. Additionally, each metric can have one or more tags in the form of key-value pairs, which provide detailed information about the metric's properties. This hierarchical structure allows for efficient storage and retrieval of data, as well as flexible querying and analysis.</p> <p>In ExaMon, the tags are used to define extra information about the sensor and some special mandatory properties of the protocol.</p> <p></p> <p>The data model resembles a hierarchical tree. In the Figure are shown the relevant sections.</p> <ul> <li>Sensor location: (Mandatory) It is a free hierarchical sequence of key/value couples used to locate the data source. At least one couple should be present.</li> <li>Plugin name: (Mandatory) It is the name of the data collector agent (plugin) which acquires data from this sensor</li> <li>Channel type: (Mandatory) specifies the type of channel<ul> <li>data: metrics values sent by the plugin</li> <li>cmd: commands sent to the sensor/plugin</li> </ul> </li> <li>Specific plugin tags: (Optional) additional free key/values couples to add custom plugin attributes that are unique for this data source.</li> </ul> <p>The data source or a sensor is effectively defined when it is possible to uniquely determine a complete path within the tag hierarchical tree, from top to bottom. This means that in ExaMon, a sensor can only have one value for each of its tags.</p> <p>Currently, to enter data within ExaMon you must connect to the transport layer (MQTT Broker) and send messages in a specific format.</p>"},{"location":"Introduction_prev/#transport","title":"Transport","text":"<p>The ExaMon data model is well-suited for use with the MQTT protocol in the transport layer.</p> <p>In MQTT, the tags and metric name define the topic, while the value and timestamp are part of the payload. This mapping fits well with the ExaMon data model, where each metric is associated with specific tags that define its context.</p> <p>By leveraging the MQTT protocol, ExaMon can efficiently transport data across distributed systems, allowing for real-time monitoring and analysis of system performance. MQTT provides a lightweight and efficient communication protocol that is well-suited for IoT and other resource-constrained systems.</p> <p></p>"},{"location":"Introduction_prev/#mqtt-topic","title":"MQTT Topic","text":"<p>The following figure shows in detail the structure of an MQTT topic implementing the ExaMon data model.</p> <p></p>"},{"location":"Introduction_prev/#mqtt-payload","title":"MQTT Payload","text":"<p>The MQTT payload, in the basic implementation of ExaMon, is a string obtained by concatenating the measured value and the timestamp associated with it.</p> <p></p>"},{"location":"Introduction_prev/#example","title":"Example","text":"<p>An example of the MQTT data that are compatible with the ExaMon data model is the following (Obtained from the reference implementation in pmu_pub):</p> Topic: org/testorg/cluster/testcluster/node/testnode00/plugin/pmu_pub/chnl/data/core/23/aperf Payload: 10822010350093;1658832078.001"},{"location":"Introduction_prev/#storage","title":"Storage","text":"<p>In the storage layer of ExaMon, the Cassandra table schema maps the data model. Cassandra tables are collections of rows, where each row has a primary key. KairosDB is used by ExaMon to define the primary keys in Cassandra and enable efficient indexing and searching of data. The primary keys are a concatenation of the metric name and tags, and they are used for data indexing and searching. The values and timestamps represent the columns of the row.</p> <p></p>"},{"location":"Introduction_prev/#query-language","title":"Query language","text":"<p>The data stored by ExaMon can be retrieved by querying the Cassandra and KairosDB modules directly. However, since the data is stored in a de-normalized form in Cassandra, managing the data from an application perspective can be complex, despite the benefits of improved scalability and performance.</p> <p>To simplify the development of applications that use ExaMon data, a uniform entry point for all data was developed and named Examon-client. This uniform interface provides a simplified and consistent view of the data, hiding the complexities of data management from the application developer.</p> <p>Examon-client allows users to access monitoring and analytics data from ExaMon using a common set of queries and provides a simpler and more intuitive way to interact with the data. In the following section, we will describe the features and capabilities of Examon-client in more detail.</p>"},{"location":"Introduction_prev/#examon-client","title":"Examon-client","text":"<p>Examon-client is a Python package that provides a uniform interface for accessing and analyzing data stored in the ExaMon database. This client enables querying of metrics from the database through a pluggable interface. In the current implementation, the client uses the KairosDB REST API to retrieve data.</p> <p>Examon-client provides a SQL-like query language for ease of use, allowing users to retrieve data simply and intuitively. The client is designed to work seamlessly with ExaMon's storage architecture, currently based on Cassandra and KairosDB.</p> <p>With Examon-client, ExaMon data can be accessed locally using the Pandas interface or in a distributed fashion using Apache Spark or Dask. This makes it possible to handle heavy workloads efficiently and effectively.</p> <p></p> <p>Queries can be built and executed with an SQL-like language.</p> <pre><code>data = sq.SELECT('*') \\\n      .FROM('p0_power') \\\n      .WHERE(cluster='marconi100' node='r255n18') \\\n      .TSTART(10,'minutes') \\\n      .execute()\n</code></pre> <p>The data returned by the query is a dataframe.</p> <p></p>"},{"location":"Introduction_prev/#data-visualization","title":"Data Visualization","text":"<p>In addition to querying data through the Examon-client, users can also access ExaMon data through the integrated visualization tool, Grafana. Grafana is a high-level component that provides live visualization of different data sources, including information about completed jobs, system service statuses, and hardware sensors' measurements.</p> <p>A wide range of views and pre-built dashboards can be accessed using the dedicated Grafana server. For example, by selecting specific nodes and a time interval (such as the last 5 minutes), users can visualize different metrics as time series and/or aggregated values (average, max/min, variance, etc.).</p> <p></p> <p>Alternatively, Grafana can be also used to gain an overall view of all the racks of the data centers, with many potential benefits, among them identifying nodes with anomalous thermal load or workload imbalance.</p> <p></p> <p>Finally, it is possible to visualize the usage of the computing resources made by the users through the metrics acquired from the job scheduler (Slurm).</p> <p></p> <p>Grafana provides an intuitive and user-friendly interface for exploring and visualizing ExaMon data, enabling users to quickly and easily gain insights into system performance and behavior. With its extensive range of features and customization options, Grafana is a powerful tool for analyzing and visualizing ExaMon data.</p>"},{"location":"contactus/","title":"Contact Us","text":"<ul> <li>Andrea Bartolini - (PI)</li> <li>Andrea Borghesi - (PI)</li> <li>Francesco Beneventi - (Developer)</li> <li>Luca Benini - (PI)</li> </ul>"},{"location":"credits/","title":"Credits","text":"<p>This work is supported by the EU FETHPC projects:</p> <ul> <li>MULTITHERMAN (g.a. 291125)</li> <li>ANTAREX (g.a. 671623)</li> <li>IOTWINS (g.a. 857191)</li> <li>REGALE (g.a. 956560)</li> <li>GRAPH MASSIVIZER (g.a. 101093202)</li> </ul>"},{"location":"getting_started/","title":"Welcome to the ExaMon Documentation","text":"<p>ExaMon is a powerful monitoring and analytics framework that helps you collect and analyze data from a variety of sources. It employs data engeenering best practices and the latest open-source tools to deliver a fully featured data analysis environment. The following documentation provides instructions for installing framework components and accessing and analyzing data.</p>"},{"location":"getting_started/#admin-documentation","title":"Admin Documentation","text":"<p>If you're an administrator of ExaMon, our admin documentation provides detailed information on how to install and configure ExaMon. Our admin documentation covers topics such as:</p> <ul> <li>Installing and configuring the ExaMon server</li> <li>Setting up data sources</li> <li>Managing users and permissions</li> <li>Configuring alerting and notifications</li> <li>Managing dashboards and reports</li> </ul> <p>Click here to access the Admin Documentation</p>"},{"location":"getting_started/#user-documentation","title":"User Documentation","text":"<p>If you're a user of ExaMon, our user documentation provides detailed information on how to access and analyze data using ExaMon. Our user documentation covers topics such as:</p> <ul> <li>Setting up the ExaMon client</li> <li>Accessing and analyzing data</li> <li>Visualizing data using charts and graphs</li> <li>Setting up alerts and notifications</li> <li>Managing your data using dashboards</li> </ul> <p>Click here to access the User Documentation</p>"},{"location":"getting_started/#community","title":"Community","text":"<p>We hope this documentation helps you get the most out of ExaMon. If you have any questions or feedback, please don't hesitate to contact us at:</p> <ul> <li>ExaMon Forum</li> </ul>"},{"location":"Administrators/Getting_started/","title":"ExaMon Docker Setup","text":"<p>This setup will install all server-side components of the ExaMon framework:</p> <ul> <li>MQTT broker and Db connector</li> <li>Grafana</li> <li>KairosDB</li> <li>Cassandra</li> </ul>"},{"location":"Administrators/Getting_started/#prerequisites","title":"Prerequisites","text":"<p>Since Cassandra is the component that requires the majority of resources, you can find more details about the suggested hardware configuration of the system that will host the services here:</p> <p>Hardware Configuration</p> <p>To install all the services needed by ExaMon we will use Docker and Docker Compose:</p> <p>Install Docker and Docker Compose.</p>"},{"location":"Administrators/Getting_started/#setup","title":"Setup","text":""},{"location":"Administrators/Getting_started/#clone-the-git-repository","title":"Clone the Git repository","text":"<p>First you will need to clone the Git repository:</p> <pre><code>git clone https://github.com/EEESlab/examon\ngit checkout develop\ncd examon/docker/\n</code></pre>"},{"location":"Administrators/Getting_started/#create-docker-services","title":"Create Docker Services","text":"<p>Once you have the above setup, you need to create the Docker services:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will build the Docker images and fetch some prebuilt images and then start the services. You can refer to the <code>docker-compose.yml</code> file to see the full configuration. </p>"},{"location":"Administrators/Getting_started/#configure-grafana","title":"Configure Grafana","text":"<p>Log in to the Grafana server using your browser and the default credentials:</p> <p>http://localhost:3000</p> <p>Follow the normal procedure for adding a new data source (KairosDB):</p> <p>Add a Datasource</p> <p>Fill out the form with the following settings:</p> <ul> <li>Type: <code>KairosDB</code> </li> <li>Name: <code>kairosdb</code> </li> <li>Url: http://kairosdb:8083 </li> <li>Access: <code>Proxy</code></li> </ul>"},{"location":"Administrators/Getting_started/#usage-examples","title":"Usage Examples","text":""},{"location":"Administrators/Getting_started/#collecting-data-using-the-pmu_pub-plugin","title":"Collecting data using the \"pmu_pub\" plugin","text":"<p>Once all Docker services are running (can be started either by <code>docker compose up -d</code> or <code>docker compose start</code>) the MQTT broker is available at <code>TEST_SERVER</code> port <code>1883</code> where <code>TEST_SERVER</code> is the address of the server where the services run.</p> <p>To test the installation, we can use the <code>pmu_pub</code> plugin available in the <code>publishers/pmu_pub</code> folder of  this project.</p> <p>After having installed and configured it on one or more test nodes we can start the data collection running for example:</p> <p><pre><code>[root@testnode00]$ ./pmu_pub -b TEST_SERVER -p 1883 -t org/myorg -s 1 run\n</code></pre> If everything went well, the data are available both through the Grafana interface and using the examon-client. </p>"},{"location":"Marconi100/Metrics_reference/","title":"Marconi 100 - CINECA","text":"<ul> <li>Model: IBM Power AC922 (Whiterspoon)</li> <li>Racks: 55 total (49 compute)</li> <li>Nodes: 980</li> <li>Processors: 2x16 cores IBM POWER9 AC922 at 2.6(3.1) GHz</li> <li>Accelerators: 4 x NVIDIA Volta V100 GPUs/node, Nvlink 2.0, 16GB</li> <li>Cores: 32 cores/node, Hyperthreading x4</li> <li>RAM: 256 GB/node (242 usable)</li> <li>Peak Performance: about 32 Pflop/s, 32 TFlops per node</li> <li>Internal Network: Mellanox IB EDR DragonFly++ 100Gb/s</li> <li>Disk Space: 8PB raw GPFS storage</li> </ul>"},{"location":"Marconi100/Metrics_reference/#metrics","title":"Metrics","text":"<p>This Section is a\u00a0brief description of some of the metrics collected by ExaMon from the Marconi100 cluster. It is intended only as an example and is therefore not exhaustive. The Marconi, Galileo and Galileo 100 clusters have similar metrics.</p>"},{"location":"Marconi100/Metrics_reference/#ipmi","title":"IPMI","text":"<p>The following table describes the metrics collected by the ipmi_pub plugin.</p> Metric Name Description Unit pX_coreY_temp Temperature of core n. Y in the CPU socket n. X. X=0..1, Y=0..23 \u00b0C dimmX_temp Temperature of DIMM module n. X. X=0..15 \u00b0C gpuX_core_temp Temperature of the core for the GPU id X. X=0,1,3,4 \u00b0C gpuX_mem_temp Temperature of the memory for the GPU id X. X=0,1,3,4 \u00b0C fanX_Y Speed of the Fan Y in module X. X=0..3, Y=0,1 RPM pX_vdd_temp Temperature of the voltage regulator for the CPU socket n. X. X=0..1 \u00b0C fan_disk_power Power consumption of the disk fan W pX_io_power Power consumption for the I/O subsystem for the CPU socket n. X. X=0..1 W pX_mem_power Power consumption for the memory subsystem for the CPU socket n. X. X=0..1 W pX_power Power consumption for the CPU socket n. X. X=0..1 W psX_input_power Power consumption at the input of power supply n. X. X=0..1 W total_power Total node power consumption W psX_input_voltag Voltage at the input of power supply n. X. X=0..1 V psX_output_volta Voltage at the output of power supply n. X. X=0..1 V psX_output_curre Current at the output of power supply n. X. X=0..1 A pcie Temperature at the PCIExpress slots \u00b0C ambient Temperature at the node inlet \u00b0C"},{"location":"Marconi100/Metrics_reference/#ganglia","title":"Ganglia","text":"<p>The following table describes the metrics collected by the ganglia_pub plugin. The data are extracted from a Ganglia^([6])\u00a0instance that CINECA runs on Marconi100.</p> Metric name Type Unit Description gexec core gexec available cpu_aidle cpu % Percent of time since boot idle CPU cpu_idle cpu % Percentage of time that the CPU or CPUs were idle and the system did not have an outstanding disk I/O request cpu_nice cpu % Percentage of CPU utilization that occurred while executing at the user level with nice priority cpu_speed cpu MHz CPU Speed in terms of MHz cpu_steal cpu % cpu_steal cpu_system cpu % Percentage of CPU utilization that occurred while executing at the system level cpu_user cpu % Percentage of CPU utilization that occurred while executing at the user level cpu_wio cpu % Percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request cpu_num disk_free disk GB Total free disk space disk_total disk GB Total available disk space part_max_used disk % Maximum percent used for all partitions load_fifteen load Fifteen minute load average load_five load Five minute load average load_one load One minute load average mem_buffers memory KB Amount of buffered memory mem_cached memory KB Amount of cached memory mem_free memory KB Amount of available memory mem_shared memory KB Amount of shared memory mem_total memory KB Total amount of memory displayed in KBs swap_free memory KB Amount of available swap memory swap_total memory KB Total amount of swap space displayed in KBs bytes_in network bytes/sec Number of bytes in per second bytes_out network bytes/sec Number of bytes out per second pkts_in network packets/sec Packets in per second pkts_out network packets/sec Packets out per second proc_run process Total number of running processes proc_total process Total number of processes boottime system s The last time that the system was started machine_type system System architecture os_name system Operating system name os_release system Operating system release date cpu_ctxt cpu ctxs/sec Context Switches cpu_intr cpu % cpu_intr cpu_sintr cpu % cpu_sintr multicpu_idle0 cpu % Percentage of CPU utilization that occurred while executing at the idle level procs_blocked cpu processes Processes blocked procs_created cpu proc/sec Number of processes and threads created disk_free_absolute_developers disk GB Disk space available (GB) on /developers disk_free_percent_developers disk % Disk space available (%) on /developers diskstat_sda_io_time diskstat s The time in seconds spent in I/O operations diskstat_sda_percent_io_time diskstat percent The percent of disk time spent on I/O operations diskstat_sda_read_bytes_per_sec diskstat bytes/sec The number of bytes read per second diskstat_sda_reads_merged diskstat reads The number of reads merged. Reads which are adjacent to each other may be merged for efficiency. Multiple reads may become one before it is handed to the disk, and it will be counted (and queued) as only one I/O. diskstat_sda_reads diskstat reads The number of reads completed diskstat_sda_read_time diskstat s The time in seconds spent reading diskstat_sda_weighted_io_time diskstat s The weighted time in seconds spend in I/O operations. This measures each I/O start, I/O completion, I/O merge, or read of these stats by the number of I/O operations in progress times the number of seconds spent doing I/O. diskstat_sda_write_bytes_per_sec diskstat bytes/sec The number of bytes written per second diskstat_sda_writes_merged diskstat writes The number of writes merged. Writes which are adjacent to each other may be merged for efficiency. Multiple writes may become one before it is handed to the disk, and it will be counted (and queued) as only one I/O. diskstat_sda_writes diskstat writes The number of writes completed diskstat_sda_write_time diskstat s The time in seconds spent writing ipmi_ambient_temp ipmi C IPMI data ipmi_avg_power ipmi Watts IPMI data ipmi_cpu1_temp ipmi C IPMI data ipmi_cpu2_temp ipmi C IPMI data ipmi_gpu_outlet_temp ipmi C IPMI data ipmi_hdd_inlet_temp ipmi C IPMI data ipmi_pch_temp ipmi C IPMI data ipmi_pci_riser_1_temp ipmi C IPMI data ipmi_pci_riser_2_temp ipmi C IPMI data ipmi_pib_ambient_temp ipmi C IPMI data mem_anonpages memory Bytes AnonPages mem_dirty memory Bytes The total amount of memory waiting to be written back to the disk. mem_hardware_corrupted memory Bytes HardwareCorrupted mem_mapped memory Bytes Mapped mem_writeback memory Bytes The total amount of memory actively being written back to the disk. vm_pgmajfault memory_vm ops/s pgmajfault vm_pgpgin memory_vm ops/s pgpgin vm_pgpgout memory_vm ops/s pgpgout vm_vmeff memory_vm pct VM efficiency rx_bytes_eth0 network bytes/sec received bytes per sec rx_drops_eth0 network pkts/sec receive packets dropped per sec rx_errs_eth0 network pkts/sec received error packets per sec rx_pkts_eth0 network pkts/sec received packets per sec tx_bytes_eth0 network bytes/sec transmitted bytes per sec tx_drops_eth0 network pkts/sec transmitted dropped packets per sec tx_errs_eth0 network pkts/sec transmitted error packets per sec tx_pkts_eth0 network pkts/sec transmitted packets per sec procstat_gmond_cpu procstat percent The total percent CPU utilization procstat_gmond_mem procstat B The total memory utilization softirq_blockiopoll softirq ops/s Soft Interrupts softirq_block softirq ops/s Soft Interrupts softirq_hi softirq ops/s Soft Interrupts softirq_hrtimer softirq ops/s Soft Interrupts softirq_netrx softirq ops/s Soft Interrupts softirq_nettx softirq ops/s Soft Interrupts softirq_rcu softirq ops/s Soft Interrupts softirq_sched softirq ops/s Soft Interrupts softirq_tasklet softirq ops/s Soft Interrupts softirq_timer softirq ops/s Soft Interrupts entropy_avail ssl bits Entropy Available tcpext_listendrops tcpext count/s listendrops tcpext_tcploss_percentage tcpext pct TCP percentage loss, tcploss / insegs + outsegs tcp_attemptfails tcp count/s attempt fails tcp_insegs tcp count/s insegs tcp_outsegs tcp count/s outsegs tcp_retrans_percentage tcp pct TCP retrans percentage, retranssegs / insegs + outsegs udp_indatagrams udp count/s indatagrams udp_inerrors udp count/s inerrors udp_outdatagrams udp count/s outdatagrams multicpu_idle16 cpu % Percentage of CPU utilization that occurred while executing at the idle level multicpu_steal16 cpu % Percentage of CPU preempted by the hypervisor multicpu_system16 cpu % Percentage of CPU utilization that occurred while executing at the system level multicpu_user16 cpu % Percentage of CPU utilization that occurred while executing at the user level multicpu_wio16 cpu % Percentage of CPU utilization that occurred while executing at the wio level diskstat_sdb_io_time diskstat s The time in seconds spent in I/O operations diskstat_sdb_percent_io_time diskstat percent The percent of disk time spent on I/O operations diskstat_sdb_read_bytes_per_sec diskstat bytes/sec The number of bytes read per second diskstat_sdb_reads_merged diskstat reads The number of reads merged. Reads which are adjacent to each other may be merged for efficiency. Multiple reads may become one before it is handed to the disk, and it will be counted (and queued) as only one I/O. diskstat_sdb_reads diskstat reads The number of reads completed diskstat_sdb_read_time diskstat s The time in seconds spent reading diskstat_sdb_weighted_io_time diskstat s The weighted time in seconds spend in I/O operations. This measures each I/O start, I/O completion, I/O merge, or read of these stats by the number of I/O operations in progress times the number of seconds spent doing I/O. diskstat_sdb_write_bytes_per_sec diskstat bytes/sec The number of bytes written per second diskstat_sdb_writes_merged diskstat writes The number of writes merged. Writes which are adjacent to each other may be merged for efficiency. Multiple writes may become one before it is handed to the disk, and it will be counted (and queued) as only one I/O. diskstat_sdb_writes diskstat writes The number of writes completed diskstat_sdb_write_time diskstat s The time in seconds spent writing GpuX_dec_utilization gpu % X=0,..,3 GpuX_enc_utilization gpu % X=0,..,3 GpuX_enforced_power_limit gpu Watts X=0,..,3 GpuX_gpu_temp gpu Celsius X=0,..,3 GpuX_low_util_violation gpu X=0,..,3 GpuX_mem_copy_utilization gpu % X=0,..,3 GpuX_mem_util_samples gpu X=0,..,3 GpuX_memory_clock gpu Mhz X=0,..,3 GpuX_memory_temp gpu Celsius X=0,..,3 GpuX_power_management_limit gpu Watts X=0,..,3 GpuX_power_usage gpu Watts X=0,..,3 GpuX_pstate gpu X=0,..,3 GpuX_reliability_violation gpu X=0,..,3 GpuX_sm_clock gpu Mhz X=0,..,3"},{"location":"Marconi100/Metrics_reference/#nagios","title":"Nagios","text":"<p>This is a description of the metrics collected by the ExaMon \"nagios_pub\" plugin. The data reflect those monitored by the Nagios^([7])\u00a0tool that currently runs in the CINECA clusters. Specifically, the plugin interfaces with a Nagios extension developed by CINECA called \"Hnagios\"^([8]). Although the monitored services and metrics are similar between all clusters, here we will specifically discuss those of Marconi100.</p>"},{"location":"Marconi100/Metrics_reference/#metrics_1","title":"Metrics","text":"<p>Currently, this plugin collects three metrics</p> name 0 hostscheduleddowtimecomments 1 plugin_output 2 state"},{"location":"Marconi100/Metrics_reference/#hostscheduleddowtimecomments","title":"Hostscheduleddowtimecomments","text":"<p>This metric is obtained from the \"Hnagios\" output and reports comments made by system administrators about the maintenance status of the specific monitored resource</p> name tag key tag values 0 hostscheduleddowtimecomments node [ems02, login03, login08, master01, master02, ... 1 hostscheduleddowtimecomments slot [01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 1... 2 hostscheduleddowtimecomments description [afs::blocked_conn::status, afs::bosserver::st... 3 hostscheduleddowtimecomments plugin [nagios_pub] 4 hostscheduleddowtimecomments chnl [data] 5 hostscheduleddowtimecomments host_group [compute, compute,cincompute, efgwcompute, efg... 6 hostscheduleddowtimecomments cluster [galileo, marconi, marconi100] 7 hostscheduleddowtimecomments state [0, 1, 2, 3] 8 hostscheduleddowtimecomments nagiosdrained [0, 1] 9 hostscheduleddowtimecomments org [cineca] 10 hostscheduleddowtimecomments state_type [0, 1] 11 hostscheduleddowtimecomments rack [205, 206, 207, 208, 209, 210, 211, 212, 213, ..."},{"location":"Marconi100/Metrics_reference/#plugin_output","title":"Plugin_output","text":"<p>This metric collects the outbound messages from Nagios agents responsible for monitoring services.</p> name tag key tag values 0 plugin_output node [ems02, ethcore01-mgt, ethcore02-mgt, gss03, g... 1 plugin_output slot [01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 1... 2 plugin_output description [EFGW_cluster::status::availability, EFGW_clus... 3 plugin_output plugin [nagios_pub] 4 plugin_output chnl [data] 5 plugin_output host_group [compute, compute,cincompute, containers, cumu... 6 plugin_output cluster [galileo, marconi, marconi100] 7 plugin_output state [0, 1, 2, 3] 8 plugin_output nagiosdrained [0, 1] 9 plugin_output org [cineca] 10 plugin_output state_type [0, 1] 11 plugin_output rack [202, 205, 206, 207, 208, 209, 210, 211, 212, ..."},{"location":"Marconi100/Metrics_reference/#state","title":"State","text":"<p>This metric collects the equivalent numerical value of the actual state of the service monitored by Nagios.</p> name tag key tag values 0 state node [ems02, ethcore01-mgt, ethcore02-mgt, gss03, g... 1 state slot [01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 1... 2 state description [EFGW_cluster::status::availability, EFGW_clus... 3 state plugin [nagios_pub] 4 state chnl [data] 5 state host_group [compute, compute,cincompute, containers, cumu... 6 state cluster [galileo, marconi, marconi100] 7 state nagiosdrained [0, 1] 8 state org [cineca] 9 state state_type [0, 1] 10 state rack [202, 205, 206, 207, 208, 209, 210, 211, 212, ..."},{"location":"Marconi100/Metrics_reference/#resources-monitored-in-marconi100","title":"Resources monitored in Marconi100","text":"<p>The name and type of the services/resources monitored by Nagios and corresponding to the metrics just described above are collected in the \"description\" tag.</p>"},{"location":"Marconi100/Metrics_reference/#nagios-checks-for-marconi100","title":"Nagios checks for Marconi100","text":"<p>In the following table is collected a brief description of the services \u00a0monitored by Nagios in the Marconi100 cluster.</p> Service/resource Description alive::ping Ping command output backup::local::status Backup service batchs::... Batch scheduler services bmc::events Events from the node BMC cluster::... Cluster availability container::... Status of the container system dev::... Node devices file::integrity Files integrity filesys::... Filesystem elements galera::... Status of the database components globus::... Status of the FTP system memory::phys::total Physical memory size monitoring::health Monitoring subsystem net::ib::status Infiniband nfs::rpc::status NFS nvidia::... GPUs service::... Misc. services ssh::... SSH server sys::... Misc. systems (GPFS,...)"},{"location":"Marconi100/Metrics_reference/#nagios-state-encoding","title":"Nagios state encoding","text":"<p>This table describes the numerical encoding of the state metric values and the state_type tag, as defined by Nagios.</p> <p>[TABLE]</p>"},{"location":"Marconi100/Metrics_reference/#_1","title":"Marconi 100","text":""},{"location":"Marconi100/Metrics_reference/#nvidia","title":"Nvidia","text":"<p>The following table describes the metrics collected by the nvidia_pub plugin.</p> <p>PLEASE NOTE\u00a0This plugin has collected data only for a short period (January/February 2020) and is currently not enabled due to CINECA policy.</p> Metric name Description Unit clock.sm Current frequency of SM (Streaming Multiprocessor) clock. MHz clocks.gr Current frequency of graphics (shader) clock. MHz clocks.mem Current frequency of memory clock. MHz clocks_throttle_reasons.active Bitmask of active clock throttle reasons. See nvml.h for more details power.draw The last measured power draw for the entire board, in watts. Only available if power management is supported. This reading is accurate to within +/- 5 watts. W temperature.gpu Core GPU temperature. in degrees C. \u00b0C"},{"location":"Marconi100/Metrics_reference/#slurm","title":"Slurm","text":"<p>Currently the job scheduler data is collected as per-job data in plain Cassandra tables.</p> <p>This is a description of the data currently stored (where available) for each executed job:</p> Table fields Description account charge to specified account accrue_time time job is eligible for running admin_comment administrator's arbitrary comment alloc_node local node and system id making the resource allocation alloc_sid local sid making resource alloc array_job_id job_id of a job array or 0 if N/A array_max_tasks Maximum number of running tasks array_task_id task_id of a job array array_task_str string expression of task IDs in this record assoc_id association id for job batch_features features required for batch script's node batch_flag 1 if batch: queued job with script batch_host name of host running batch script billable_tres billable TRES cache. updated upon resize bitflags Various job flags boards_per_node boards per node required by job burst_buffer burst buffer specifications burst_buffer_state burst buffer state info command command to be executed, built from submitted \u00a0job's argv and NULL for salloc command comment arbitrary comment contiguous 1 if job requires contiguous nodes core_spec specialized core count cores_per_socket cores per socket required by job cpu_freq_gov cpu frequency governor cpu_freq_max Maximum cpu frequency cpu_freq_min Minimum cpu frequency cpus_alloc_layout map: list of cpu allocated per node cpus_allocated map: number of cpu allocated per node cpus_per_task number of processors required for each task cpus_per_tres semicolon delimited list of TRES=# values dependency synchronize job execution with other jobs derived_ec highest exit code of all job steps eligible_time time job is eligible for running end_time time of termination, actual or expected exc_nodes comma separated list of excluded nodes exit_code exit code for job (status from wait call) features comma separated list of required features group_id group job submitted as job_id job ID job_state state of the job, see enum job_states last_sched_eval last time job was evaluated for scheduling licenses licenses required by the job max_cpus maximum number of cpus usable by job max_nodes maximum number of nodes usable by job mem_per_cpu boolean mem_per_node boolean mem_per_tres semicolon delimited list of TRES=# values min_memory_cpu minimum real memory required per allocated CPU min_memory_node minimum real memory required per node name name of the job network network specification nice requested priority change nodes list of nodes allocated to job ntasks_per_board number of tasks to invoke on each board ntasks_per_core number of tasks to invoke on each core ntasks_per_core_str number of tasks to invoke on each core \u00a0as string ntasks_per_node number of tasks to invoke on each node ntasks_per_socket number of tasks to invoke on each socket ntasks_per_socket_str number of tasks to invoke on each socket as string num_cpus minimum number of cpus required by job num_nodes minimum number of nodes required by job partition name of assigned partition pn_min_cpus minimum # CPUs per node, default=0 pn_min_memory minimum real memory per node, default=0 pn_min_tmp_disk minimum tmp disk per node, default=0 power_flags power management flags, \u00a0see SLURM_POWERFLAGS pre_sus_time time job ran prior to last suspend preempt_time preemption signal time priority relative priority of the job, 0=held, 1=required nodes DOWN/DRAINED profile Level of acct_gather_profile qos Quality of Service reboot node reboot requested before start req_nodes comma separated list of required nodes req_switch Minimum number of switches requeue enable or disable job requeue option resize_time time of latest size change restart_cnt count of job restarts resv_name reservation name run_time job run time (seconds) run_time_str job run time (seconds) as string sched_nodes list of nodes scheduled to be used for job shared 1 if job can share nodes with other jobs show_flags conveys level of details requested sockets_per_board sockets per board required by job sockets_per_node sockets per node required by job start_time time execution begins, actual or expected state_reason reason job still pending or failed, see slurm.h:enum job_state_reason std_err pathname of job's stderr file std_in pathname of job's stdin file std_out pathname of job's stdout file submit_time time of job submission suspend_time time job last suspended or resumed system_comment slurmctld's arbitrary comment threads_per_core threads per core required by job time_limit maximum run time in minutes or INFINITE time_limit_str maximum run time in minutes or INFINITE as string time_min minimum run time in minutes or INFINITE tres_alloc_str tres used in the job as string tres_bind Task to TRES binding directives tres_freq TRES frequency directives tres_per_job semicolon delimited list of TRES=# values tres_per_node semicolon delimited list of TRES=# values tres_per_socket semicolon delimited list of TRES=# values tres_per_task semicolon delimited list of TRES=# values tres_req_str tres requested in the job as string user_id user the job runs as wait4switch Maximum time to wait for minimum switches wckey wckey for job work_dir pathname of working directory"},{"location":"MonteCimone/Examon_Monte_Cimone/","title":"Getting started","text":"<p>Introductory notebook for getting started with ExaMon and the Monte Cimone RISC-V cluster.</p> In\u00a0[1]: Copied! <pre>%matplotlib inline\n\n# ssh -L 3000:192.168.1.201:3000 -L 5000:192.168.1.201:5000 -p 2223 &lt;mc_username&gt;@137.204.56.52\n\n\nimport os\nimport numpy as np\n\nimport pandas as pd\nfrom examon.examon import Client, ExamonQL\n\n# Connect\nUSER = 'ext_student'\nPWD = 'ext_student'\nex = Client('127.0.0.1', port='3000', user=USER, password=PWD, verbose=False, proxy=True)\nsq = ExamonQL(ex)\n</pre> %matplotlib inline  # ssh -L 3000:192.168.1.201:3000 -L 5000:192.168.1.201:5000 -p 2223 @137.204.56.52   import os import numpy as np  import pandas as pd from examon.examon import Client, ExamonQL  # Connect USER = 'ext_student' PWD = 'ext_student' ex = Client('127.0.0.1', port='3000', user=USER, password=PWD, verbose=False, proxy=True) sq = ExamonQL(ex) In\u00a0[3]: Copied! <pre>pd.DataFrame(sq.metric_list)\n</pre> pd.DataFrame(sq.metric_list) Out[3]: name 0 CYCLES 1 INSTRUCTIONS 2 dsk_total.read 3 dsk_total.writ 4 io_total.read 5 io_total.writ 6 load_avg.15m 7 load_avg.1m 8 load_avg.5m 9 memory_usage.buff 10 memory_usage.cach 11 memory_usage.free 12 memory_usage.used 13 net_total.recv 14 net_total.send 15 paging.in 16 paging.out 17 procs.blk 18 procs.new 19 procs.run 20 system.csw 21 system.int 22 temperature.average 23 temperature.cpu_temp 24 temperature.mb_temp 25 temperature.nvme_temp 26 temperature.total 27 total_cpu_usage.idl 28 total_cpu_usage.stl 29 total_cpu_usage.sys 30 total_cpu_usage.usr 31 total_cpu_usage.wai In\u00a0[5]: Copied! <pre>df = sq.DESCRIBE(metric='INSTRUCTIONS') \\\n    .execute()\n    \ndf\n</pre> df = sq.DESCRIBE(metric='INSTRUCTIONS') \\     .execute()      df Out[5]: name tag key tag values 0 INSTRUCTIONS node [mcimone-node-1, mcimone-node-2, mcimone-node-... 1 INSTRUCTIONS core [0, 1, 2, 3] 2 INSTRUCTIONS plugin [pmu_pub] 3 INSTRUCTIONS chnl [data] 4 INSTRUCTIONS cluster [hifive] 5 INSTRUCTIONS org [unibo] In\u00a0[22]: Copied! <pre>data = sq.SELECT('node','cluster','core') \\\n    .FROM('INSTRUCTIONS') \\\n    .TSTART(30, 'minutes') \\\n    .execute()\n    \ndata.df_table.head(10)\n</pre> data = sq.SELECT('node','cluster','core') \\     .FROM('INSTRUCTIONS') \\     .TSTART(30, 'minutes') \\     .execute()      data.df_table.head(10) Out[22]: cluster core name node timestamp value 0 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:52+02:00 1.794506e+11 1 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:52.500000+02:00 1.794748e+11 2 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:53+02:00 1.794753e+11 3 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:53.500000+02:00 1.794758e+11 4 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:54+02:00 1.794764e+11 5 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:54.500000+02:00 1.794769e+11 6 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:55+02:00 1.794775e+11 7 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:55.500000+02:00 1.794780e+11 8 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:56+02:00 1.794786e+11 9 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-27 19:20:56.500000+02:00 1.794791e+11 In\u00a0[24]: Copied! <pre>data.to_series(flat_index=True, interp='time', dropna=True).df_ts.plot(figsize=[15,30], subplots=True);\n</pre> data.to_series(flat_index=True, interp='time', dropna=True).df_ts.plot(figsize=[15,30], subplots=True); In\u00a0[36]: Copied! <pre>import json\n\n# Setup \nsq.jc.JOB_TABLES.extend(['job_info_hifive'])\n\ndata = sq.SELECT('name','user_id','job_id','job_state','start_time','end_time','nodes','num_nodes','num_cpus','work_dir') \\\n    .FROM('job_info_hifive') \\\n    .WHERE(node='mcimone-node-1') \\\n    .TSTART('27-06-2023 08:09:00') \\\n    .TSTOP('28-06-2023 23:09:00') \\\n    .execute()  \n\ndf = pd.DataFrame(json.loads(data))\ndf.head(50)\n</pre> import json  # Setup  sq.jc.JOB_TABLES.extend(['job_info_hifive'])  data = sq.SELECT('name','user_id','job_id','job_state','start_time','end_time','nodes','num_nodes','num_cpus','work_dir') \\     .FROM('job_info_hifive') \\     .WHERE(node='mcimone-node-1') \\     .TSTART('27-06-2023 08:09:00') \\     .TSTOP('28-06-2023 23:09:00') \\     .execute()    df = pd.DataFrame(json.loads(data)) df.head(50) Out[36]: end_time job_id job_state name nodes num_cpus num_nodes start_time user_id work_dir 0 2023-06-27T18:18:13.000Z 2825 CANCELLED bash mcimone-node-1 1 1 2023-06-27T18:00:08.000Z 2001 /home/abartolini/HPL/src/hpl-2.3 1 2023-06-28T11:11:47.000Z 2868 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:11:47.000Z 6010 /home/userdeiphd10 2 2023-06-27T14:48:34.000Z 2821 FAILED hpl mcimone-node-1 2 1 2023-06-27T14:48:32.000Z 2001 /home/abartolini 3 2023-06-28T11:15:55.000Z 2872 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:15:53.000Z 6008 /home/userdeiphd08 4 2023-06-28T07:05:00.000Z 2860 COMPLETED bash mcimone-node-1 1 1 2023-06-28T07:04:54.000Z 6001 /home/userdeiphd01 5 2023-06-27T18:19:49.000Z 2831 FAILED hpl mcimone-node-1 2 1 2023-06-27T18:19:46.000Z 2001 /home/abartolini 6 2023-06-28T11:32:38.000Z 2887 COMPLETED sleep mcimone-node-1 1 1 2023-06-28T11:32:08.000Z 6005 /home/userdeiphd05 7 2023-06-28T11:27:32.000Z 2877 COMPLETED bash mcimone-node-1 1 1 2023-06-28T11:27:26.000Z 6008 /home/userdeiphd08/02_ex 8 2023-06-28T11:19:44.000Z 2874 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:19:33.000Z 6010 /home/userdeiphd10 9 2023-06-27T21:11:24.000Z 2845 COMPLETED stream mcimone-node-1 4 1 2023-06-27T21:11:05.000Z 2001 /home/abartolini 10 2023-06-28T07:34:06.000Z 2864 FAILED hpl mcimone-node-1 2 1 2023-06-28T07:34:03.000Z 2001 /home/abartolini 11 2023-06-28T11:31:14.000Z 2882 COMPLETED sleep mcimone-node-1 1 1 2023-06-28T11:31:03.000Z 6011 /home/userdeiphd11 12 2023-06-28T11:13:29.000Z 2869 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:13:28.000Z 6008 /home/userdeiphd08 13 2023-06-28T11:09:07.000Z 2867 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:09:06.000Z 6008 /home/userdeiphd08 14 2023-06-28T11:30:23.000Z 2878 COMPLETED 07smc.sh mcimone-node-1 1 1 2023-06-28T11:30:22.000Z 6007 /home/userdeiphd07 15 2023-06-28T11:36:17.000Z 2901 COMPLETED test.sh mcimone-node-1 1 1 2023-06-28T11:36:16.000Z 6001 /home/userdeiphd01 16 2023-06-28T11:38:31.000Z 2903 COMPLETED 07j.sh mcimone-node-1 1 1 2023-06-28T11:38:29.000Z 6007 /home/userdeiphd07 17 2023-06-28T11:33:56.000Z 2893 COMPLETED echo mcimone-node-1 1 1 2023-06-28T11:33:56.000Z 6011 /home/userdeiphd11 18 2023-06-27T22:03:38.000Z 2855 COMPLETED hpl mcimone-node-1 4 1 2023-06-27T22:03:33.000Z 2001 /home/abartolini 19 2023-06-28T11:33:42.000Z 2892 COMPLETED echo mcimone-node-1 1 1 2023-06-28T11:33:41.000Z 6003 /home/userdeiphd03 20 2023-06-28T11:33:41.000Z 2890 COMPLETED echo mcimone-node-1 1 1 2023-06-28T11:33:40.000Z 6003 /home/userdeiphd03 21 2023-06-28T11:32:08.000Z 2885 COMPLETED echo mcimone-node-1 1 1 2023-06-28T11:32:07.000Z 6005 /home/userdeiphd05 22 2023-06-28T11:34:27.000Z 2895 COMPLETED sleep mcimone-node-1 1 1 2023-06-28T11:33:57.000Z 6011 /home/userdeiphd11 23 2023-06-28T07:30:40.000Z 2861 COMPLETED bash mcimone-node-1 1 1 2023-06-28T07:05:12.000Z 2001 /home/abartolini 24 2023-06-28T08:44:09.000Z 2866 CANCELLED hpl mcimone-node-1 4 1 2023-06-28T07:38:27.000Z 2001 /home/abartolini 25 2023-06-27T21:41:14.000Z 2854 COMPLETED stream mcimone-node-1 1 1 2023-06-27T21:40:57.000Z 2001 /home/abartolini 26 2023-06-28T07:36:11.000Z 2865 COMPLETED hpl mcimone-node-1 4 1 2023-06-28T07:36:06.000Z 2001 /home/abartolini 27 2023-06-28T11:31:03.000Z 2880 COMPLETED echo mcimone-node-1 1 1 2023-06-28T11:31:02.000Z 6011 /home/userdeiphd11 28 2023-06-28T11:30:49.000Z 2879 FAILED bash mcimone-node-1 1 1 2023-06-28T11:30:23.000Z 6008 /home/userdeiphd08/02_ex 29 2023-06-28T07:04:37.000Z 2859 COMPLETED bash mcimone-node-1 1 1 2023-06-28T07:04:26.000Z 2001 /home/abartolini 30 2023-06-27T14:48:11.000Z 2820 FAILED run_hpl.sh mcimone-node-1 1 1 2023-06-27T14:48:11.000Z 2001 /home/abartolini 31 2023-06-27T21:38:29.000Z 2852 COMPLETED stream mcimone-node-1 1 1 2023-06-27T21:38:11.000Z 2001 /home/abartolini 32 2023-06-28T11:19:09.000Z 2873 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:19:07.000Z 6010 /home/userdeiphd10 33 2023-06-27T21:13:04.000Z 2846 COMPLETED stream mcimone-node-1 4 1 2023-06-27T21:12:47.000Z 2001 /home/abartolini 34 2023-06-27T21:18:39.000Z 2851 COMPLETED stream mcimone-node-1 1 1 2023-06-27T21:18:22.000Z 2001 /home/abartolini 35 2023-06-28T11:44:48.000Z 2905 FAILED Test1 mcimone-node-1 1 1 2023-06-28T11:44:47.000Z 6004 /home/userdeiphd04 36 2023-06-28T11:14:08.000Z 2871 COMPLETED test mcimone-node-1 4 1 2023-06-28T11:14:07.000Z 6008 /home/userdeiphd08 37 2023-06-27T14:50:37.000Z 2822 FAILED hpl mcimone-node-1 2 1 2023-06-27T14:50:34.000Z 2001 /home/abartolini In\u00a0[41]: Copied! <pre>import json\n\n\ndata = sq.SELECT('name','user_id','job_id','job_state','start_time','end_time','nodes','num_nodes','num_cpus','work_dir') \\\n    .FROM('job_info_hifive') \\\n    .WHERE(job_id='2866') \\\n    .TSTART('27-06-2023 08:09:00') \\\n    .execute()  \n\ndf = pd.DataFrame(json.loads(data))\ndf.head()\n</pre> import json   data = sq.SELECT('name','user_id','job_id','job_state','start_time','end_time','nodes','num_nodes','num_cpus','work_dir') \\     .FROM('job_info_hifive') \\     .WHERE(job_id='2866') \\     .TSTART('27-06-2023 08:09:00') \\     .execute()    df = pd.DataFrame(json.loads(data)) df.head() Out[41]: end_time job_id job_state name nodes num_cpus num_nodes start_time user_id work_dir 0 2023-06-28T08:44:09.000Z 2866 CANCELLED hpl mcimone-node-1 4 1 2023-06-28T07:38:27.000Z 2001 /home/abartolini In\u00a0[69]: Copied! <pre>data = sq.SELECT('node','cluster','core') \\\n    .FROM('INSTRUCTIONS') \\\n    .WHERE(node='mcimone-node-1') \\\n    .TSTART('28-06-2023 07:38:27') \\\n    .TSTOP('28-06-2023 08:44:09') \\\n    .execute()\n    \ndata.df_table.head(10)\n</pre> data = sq.SELECT('node','cluster','core') \\     .FROM('INSTRUCTIONS') \\     .WHERE(node='mcimone-node-1') \\     .TSTART('28-06-2023 07:38:27') \\     .TSTOP('28-06-2023 08:44:09') \\     .execute()      data.df_table.head(10) Out[69]: cluster core name node timestamp value 0 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:27+02:00 3.100586e+11 1 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:27.500000+02:00 3.100629e+11 2 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:28+02:00 3.100923e+11 3 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:28.500000+02:00 3.100927e+11 4 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:29+02:00 3.100957e+11 5 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:29.500000+02:00 3.101099e+11 6 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:30+02:00 3.101479e+11 7 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:30.500000+02:00 3.103973e+11 8 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:31+02:00 3.108438e+11 9 hifive 0 INSTRUCTIONS mcimone-node-1 2023-06-28 07:38:31.500000+02:00 3.113999e+11 In\u00a0[46]: Copied! <pre>data.df_table.info()\n</pre> data.df_table.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 252320 entries, 0 to 252319\nData columns (total 6 columns):\ncluster      252320 non-null object\ncore         252320 non-null object\nname         252320 non-null object\nnode         252320 non-null object\ntimestamp    252320 non-null datetime64[ns, Europe/Rome]\nvalue        252320 non-null float64\ndtypes: datetime64[ns, Europe/Rome](1), float64(1), object(4)\nmemory usage: 11.6+ MB\n</pre> In\u00a0[70]: Copied! <pre>df = data.df_table\n\n# Sort the DataFrame by 'timestamp'\ndf = df.sort_values('timestamp')\n\n# Calculate the time difference between consecutive rows for each core and node\ndf['time_diff'] = df.groupby(['core', 'node'])['timestamp'].diff()\n\n# Calculate the Instructions per second for each core and node\ndf['instructions_per_second'] = df.groupby(['core', 'node'])['value'].diff() / df['time_diff'].dt.total_seconds()\n\n# Drop rows with NaN values (first row for each core and node)\ndf = df.dropna()\n\n# Print the resulting DataFrame\nprint(df[['cluster', 'core', 'node', 'instructions_per_second']])\n</pre> df = data.df_table  # Sort the DataFrame by 'timestamp' df = df.sort_values('timestamp')  # Calculate the time difference between consecutive rows for each core and node df['time_diff'] = df.groupby(['core', 'node'])['timestamp'].diff()  # Calculate the Instructions per second for each core and node df['instructions_per_second'] = df.groupby(['core', 'node'])['value'].diff() / df['time_diff'].dt.total_seconds()  # Drop rows with NaN values (first row for each core and node) df = df.dropna()  # Print the resulting DataFrame print(df[['cluster', 'core', 'node', 'instructions_per_second']]) <pre>      cluster core            node  instructions_per_second\n23656  hifive    3  mcimone-node-1               38523114.0\n1      hifive    0  mcimone-node-1                8620240.0\n7886   hifive    1  mcimone-node-1               14623376.0\n15771  hifive    2  mcimone-node-1               14995134.0\n7887   hifive    1  mcimone-node-1               83411746.0\n23657  hifive    3  mcimone-node-1               11415616.0\n2      hifive    0  mcimone-node-1               58787754.0\n15772  hifive    2  mcimone-node-1               14676376.0\n23658  hifive    3  mcimone-node-1                1645138.0\n7888   hifive    1  mcimone-node-1                 350340.0\n3      hifive    0  mcimone-node-1                 803444.0\n15773  hifive    2  mcimone-node-1                1136978.0\n23659  hifive    3  mcimone-node-1                2783320.0\n7889   hifive    1  mcimone-node-1                5273074.0\n15774  hifive    2  mcimone-node-1                2275020.0\n4      hifive    0  mcimone-node-1                6114222.0\n23660  hifive    3  mcimone-node-1              376575758.0\n5      hifive    0  mcimone-node-1               28421320.0\n15775  hifive    2  mcimone-node-1               21301446.0\n7890   hifive    1  mcimone-node-1              140078054.0\n15776  hifive    2  mcimone-node-1               11004830.0\n6      hifive    0  mcimone-node-1               76035596.0\n23661  hifive    3  mcimone-node-1              188484324.0\n7891   hifive    1  mcimone-node-1              271213162.0\n7892   hifive    1  mcimone-node-1              475208026.0\n15777  hifive    2  mcimone-node-1              483062156.0\n23662  hifive    3  mcimone-node-1              486469840.0\n7      hifive    0  mcimone-node-1              498794364.0\n7893   hifive    1  mcimone-node-1              885180146.0\n23663  hifive    3  mcimone-node-1              894075140.0\n...       ...  ...             ...                      ...\n23647  hifive    2  mcimone-node-1              233339268.0\n15762  hifive    1  mcimone-node-1              228270452.0\n23648  hifive    2  mcimone-node-1              217027384.0\n7878   hifive    0  mcimone-node-1              213077746.0\n31533  hifive    3  mcimone-node-1              218002042.0\n15763  hifive    1  mcimone-node-1              221205918.0\n15764  hifive    1  mcimone-node-1              221965008.0\n31534  hifive    3  mcimone-node-1              233791354.0\n23649  hifive    2  mcimone-node-1              223936646.0\n7879   hifive    0  mcimone-node-1              201658966.0\n31535  hifive    3  mcimone-node-1              209675462.0\n23650  hifive    2  mcimone-node-1              217535866.0\n15765  hifive    1  mcimone-node-1              221215852.0\n7880   hifive    0  mcimone-node-1              209611952.0\n23651  hifive    2  mcimone-node-1              266259612.0\n15766  hifive    1  mcimone-node-1              241839476.0\n7881   hifive    0  mcimone-node-1              251117610.0\n31536  hifive    3  mcimone-node-1              233599118.0\n23652  hifive    2  mcimone-node-1              226341798.0\n31537  hifive    3  mcimone-node-1              224304384.0\n7882   hifive    0  mcimone-node-1              207978252.0\n15767  hifive    1  mcimone-node-1              224403098.0\n23653  hifive    2  mcimone-node-1              223050652.0\n31538  hifive    3  mcimone-node-1              223850414.0\n15768  hifive    1  mcimone-node-1              219587870.0\n7883   hifive    0  mcimone-node-1              223695594.0\n15769  hifive    1  mcimone-node-1              225851464.0\n7884   hifive    0  mcimone-node-1              207533258.0\n23654  hifive    2  mcimone-node-1              220584236.0\n31539  hifive    3  mcimone-node-1              224012882.0\n\n[31536 rows x 4 columns]\n</pre> In\u00a0[71]: Copied! <pre>df[['timestamp','node','core','instructions_per_second']]\\\n.pivot_table(index='timestamp', columns=['node','core'], dropna=True, aggfunc='first')\\\n.plot(figsize=[15,30], subplots=True);\n</pre> df[['timestamp','node','core','instructions_per_second']]\\ .pivot_table(index='timestamp', columns=['node','core'], dropna=True, aggfunc='first')\\ .plot(figsize=[15,30], subplots=True);"},{"location":"MonteCimone/Examon_Monte_Cimone/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Ability to connect with an account (ssh) to MonteCimone</li> <li>Web browser</li> </ul>"},{"location":"MonteCimone/Examon_Monte_Cimone/#to-access-the-grafana-instance-via-the-browser","title":"To access the Grafana instance via the browser\u00b6","text":"<ul> <li>On your laptop/workstation, create a tunnel with your MC user using the following command:<pre>ssh -L 3000:localhost:3000 -L 5000:localhost:5000 -p 2223 &lt;your_mc_username&gt;@137.204.56.52\n</pre> </li> <li>Open your web browser and go to the following page:<ul> <li>http://localhost:3000/</li> </ul> </li> <li>Enter the following credentials to access the dashboard:<pre>User: ext_student\nPassword: ext_student\n</pre> </li> <li>Once logged in, you will be in the HOME page. From there, you can open the example dashboard by visiting the following link:<ul> <li>http://localhost:3000/d/PaU3WSt7z/montecimone-overview?orgId=1</li> </ul> </li> </ul>"},{"location":"MonteCimone/Examon_Monte_Cimone/#to-access-the-same-data-via-scriptnotebook","title":"To access the same data via script/notebook\u00b6","text":"<ul> <li><p>Prerequisites:</p> <ul> <li>In addition to the previous prerequisites, the ability to run a jupyter server (py3) on your laptop/workstation</li> </ul> </li> <li><p>On your laptop, start:</p> <ul> <li>a tunnel as in the previous step</li> <li>a python 3 jupyter server.</li> </ul> </li> <li><p>To access the db, the examon-client is required</p> <ul> <li>it is installed directly in the notebook by executing, once only, in a cell:<pre>- ! pip install https://github.com/fbeneventi/releases/releases/latest/download/examon-client.zip\n</pre> </li> </ul> </li> </ul>"},{"location":"MonteCimone/MonteCimone/","title":"Monte Cimone - UniBO","text":""},{"location":"MonteCimone/MonteCimone/#configuration-of-the-monte-cimone-risc-v-cluster","title":"Configuration of the Monte Cimone RISC-V cluster:","text":"<ul> <li>Manufacturer: E4</li> <li>Form factor: 1U</li> <li>Nodes: 8</li> <li>Blade configuration: Four blades with dual boards</li> <li>Motherboard: HiFive Unmatched developed by SiFive</li> <li>SoC: Freedom U740</li> <li>Cores: Four U74 cores at 1.4GHz and one S7 core with Mix+Match technology</li> <li>Cache: 2MB L2 cache</li> <li>Memory: 16GB DDR4-1866</li> <li>Storage: 1TB NVMe SSD</li> </ul>"},{"location":"MonteCimone/MonteCimone/#metrics","title":"Metrics","text":"Metric Description Unit of Measurement CYCLES Number of cycles cycles INSTRUCTIONS Number of instructions executed count dsk_total.read Total disk read operations operations dsk_total.writ Total disk write operations operations io_total.read Total input/output read operations operations io_total.writ Total input/output write operations operations load_avg.15m Load average over 15 minutes load average load_avg.1m Load average over 1 minute load average load_avg.5m Load average over 5 minutes load average memory_usage.buff Memory used for buffering bytes memory_usage.cach Memory used for caching bytes memory_usage.free Free memory available bytes memory_usage.used Memory currently in use bytes net_total.recv Total network data received bytes net_total.send Total network data sent bytes paging.in Paging operations in operations paging.out Paging operations out operations procs.blk Number of processes blocked count procs.new Number of new processes count procs.run Number of running processes count system.csw Number of context switches count system.int Number of interrupts count temperature.average Average system temperature Celsius temperature.cpu_temp CPU temperature Celsius temperature.mb_temp Motherboard temperature Celsius temperature.nvme_temp NVMe device temperature Celsius temperature.total Total system temperature Celsius total_cpu_usage.idl CPU idle time percentage total_cpu_usage.stl CPU steal time percentage total_cpu_usage.sys CPU system time percentage total_cpu_usage.usr CPU user time percentage total_cpu_usage.wai CPU wait time percentage"},{"location":"Users/Demo_ExamonQL/","title":"Exploring the ExaMon database","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n\n# Mount Drive and install the examon-client\n#\n# Mounting Drive is an optional step but heavily suggested to have optimal\n# performance in Google Colab\n\n# (optional)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n# Create and change to the Examon workspace folder (optional)\n! mkdir -p /content/drive/MyDrive/examon_workdir\n%cd /content/drive/MyDrive/examon_workdir\n\n# Install (required)\n! pip install https://github.com/fbeneventi/releases/releases/latest/download/examon-client.zip\n</pre> %matplotlib inline  # Mount Drive and install the examon-client # # Mounting Drive is an optional step but heavily suggested to have optimal # performance in Google Colab  # (optional) from google.colab import drive drive.mount('/content/drive') # Create and change to the Examon workspace folder (optional) ! mkdir -p /content/drive/MyDrive/examon_workdir %cd /content/drive/MyDrive/examon_workdir  # Install (required) ! pip install https://github.com/fbeneventi/releases/releases/latest/download/examon-client.zip <pre>Mounted at /content/drive\n/content/drive/MyDrive/examon_workdir\nCollecting https://github.com/fbeneventi/releases/releases/latest/download/examon-client.zip\n  Downloading https://github.com/fbeneventi/releases/releases/latest/download/examon-client.zip (353 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 353.9/353.9 kB 5.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from examon-client==0.4.0b1) (2023.3.post1)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from examon-client==0.4.0b1) (5.3.1)\nRequirement already satisfied: pandas&gt;=0.20.0 in /usr/local/lib/python3.10/dist-packages (from examon-client==0.4.0b1) (1.5.3)\nRequirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (from examon-client==0.4.0b1) (2023.8.1)\nCollecting diskcache&gt;=5.2.1 (from examon-client==0.4.0b1)\n  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 45.5/45.5 kB 1.4 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.20.0-&gt;examon-client==0.4.0b1) (2.8.2)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.20.0-&gt;examon-client==0.4.0b1) (1.23.5)\nRequirement already satisfied: click&gt;=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (8.1.7)\nRequirement already satisfied: cloudpickle&gt;=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (2.2.1)\nRequirement already satisfied: fsspec&gt;=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (2023.6.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (23.1)\nRequirement already satisfied: partd&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (1.4.0)\nRequirement already satisfied: pyyaml&gt;=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (6.0.1)\nRequirement already satisfied: toolz&gt;=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (0.12.0)\nRequirement already satisfied: importlib-metadata&gt;=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (6.8.0)\nRequirement already satisfied: pyarrow&gt;=7.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (9.0.0)\nCollecting lz4&gt;=4.3.2 (from dask[complete]-&gt;examon-client==0.4.0b1)\n  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 24.8 MB/s eta 0:00:00\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata&gt;=4.13.0-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (3.16.2)\nRequirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd&gt;=1.2.0-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (1.0.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=0.20.0-&gt;examon-client==0.4.0b1) (1.16.0)\nRequirement already satisfied: bokeh&gt;=2.4.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (3.2.2)\nRequirement already satisfied: jinja2&gt;=2.10.3 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (3.1.2)\nRequirement already satisfied: distributed==2023.8.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]-&gt;examon-client==0.4.0b1) (2023.8.1)\nRequirement already satisfied: msgpack&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (1.0.5)\nRequirement already satisfied: psutil&gt;=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (5.9.5)\nRequirement already satisfied: sortedcontainers&gt;=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (2.4.0)\nRequirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (2.0.0)\nRequirement already satisfied: tornado&gt;=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (6.3.2)\nRequirement already satisfied: urllib3&gt;=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (2.0.4)\nRequirement already satisfied: zict&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (3.0.0)\nRequirement already satisfied: contourpy&gt;=1 in /usr/local/lib/python3.10/dist-packages (from bokeh&gt;=2.4.2-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (1.1.0)\nRequirement already satisfied: pillow&gt;=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh&gt;=2.4.2-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (9.4.0)\nRequirement already satisfied: xyzservices&gt;=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh&gt;=2.4.2-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (2023.7.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2&gt;=2.10.3-&gt;dask[complete]-&gt;examon-client==0.4.0b1) (2.1.3)\nBuilding wheels for collected packages: examon-client\n  Building wheel for examon-client (setup.py) ... done\n  Created wheel for examon-client: filename=examon_client-0.4.0b1-py3-none-any.whl size=18778 sha256=343c1a6258da8c47ece95cb0756ca482f00ed6528ac31662e7d26e38f29ead11\n  Stored in directory: /root/.cache/pip/wheels/80/40/5e/6415332cf365491ebec39e418f3ddfaf3cea94b01e5bc54f79\nSuccessfully built examon-client\nInstalling collected packages: lz4, diskcache, examon-client\nSuccessfully installed diskcache-5.6.3 examon-client-0.4.0b1 lz4-4.3.2\n</pre> In\u00a0[\u00a0]: Copied! <pre># Init steps\n\nimport os\nimport getpass\nimport numpy as np\nimport pandas as pd\nfrom examon.examon import Client, ExamonQL\n\n# Connect\nUSER = input('username:')\nprint('password:')\nPWD = getpass.getpass()\nex = Client('examon.cineca.it', port='3002', user=USER, password=PWD, verbose=False, proxy=True)\nprint('Creating the local metadata cache (one-time task). Please wait ...')\nsq = ExamonQL(ex)\n</pre> # Init steps  import os import getpass import numpy as np import pandas as pd from examon.examon import Client, ExamonQL  # Connect USER = input('username:') print('password:') PWD = getpass.getpass() ex = Client('examon.cineca.it', port='3002', user=USER, password=PWD, verbose=False, proxy=True) print('Creating the local metadata cache (one-time task). Please wait ...') sq = ExamonQL(ex) <pre>username:admin\npassword:\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nCreating the local metadata cache (one-time task). Please wait ...\n</pre> In\u00a0[\u00a0]: Copied! <pre>display(pd.DataFrame(sq.metric_list))\n</pre> display(pd.DataFrame(sq.metric_list))  name 0 0_0 1 12V 2 1U_Stg_HDD0_Pres 3 1U_Stg_HDD1_Pres 4 1U_Stg_HDD2_Pres ... ... 2318 vm_pgpgin 2319 vm_pgpgout 2320 vm_vmeff 2321 wind_deg 2322 wind_speed <p>2323 rows \u00d7 1 columns</p> In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE() \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE() \\     .execute()  display(df) name tag keys 0 0_0 [chnl, cluster, node, org, plugin, rack, slot,... 1 1U_Stg_HDD1_Pres [chnl, cluster, health, node, org, plugin, type] 2 12V [chnl, cluster, health, node, org, plugin, typ... 3 1U_Stg_HDD0_Pres [chnl, cluster, health, node, org, plugin, type] 4 1U_Stg_HDD3_Pres [chnl, cluster, health, node, org, plugin, type] ... ... ... 2317 vm_pgmajfault [chnl, cluster, gcluster, group, node, org, pl... 2318 state [chnl, cluster, description, host_group, nagio... 2319 swap_total [chnl, cluster, gcluster, group, node, org, pl... 2320 swap_free [chnl, cluster, gcluster, group, node, org, pl... 2321 PS1_Temperature [chnl, cluster, health, node, org, part, plugi... <p>2322 rows \u00d7 2 columns</p> <p>The database contains this number of valid metric names:</p> In\u00a0[\u00a0]: Copied! <pre>df.shape[0]\n</pre> df.shape[0] Out[\u00a0]: <pre>2322</pre> <p>To get an entry from the table:</p> In\u00a0[\u00a0]: Copied! <pre>df[df.name == 'Ambient_Temp']['tag keys'].values[0]\n</pre> df[df.name == 'Ambient_Temp']['tag keys'].values[0] Out[\u00a0]: <pre>['chnl', 'cluster', 'health', 'node', 'org', 'part', 'plugin', 'type', 'units']</pre> In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(metric='CPU_Utilization') \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(metric='CPU_Utilization') \\     .execute()  display(df) name tag key tag values 0 CPU_Utilization chnl [data] 1 CPU_Utilization cluster [galileo, marconi] 2 CPU_Utilization health [ok] 3 CPU_Utilization node [node001, node002, node003, node004, node005, ... 4 CPU_Utilization org [cineca] 5 CPU_Utilization part [knl, skylake] 6 CPU_Utilization plugin [confluent_pub, ipmi_pub] 7 CPU_Utilization type [Other] In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(tag_key = 'plugin') \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(tag_key = 'plugin') \\     .execute()  display(df) tag values 0 ipmi_pub 1 confluent_pub 2 vertiv_pub 3 schneider_pub 4 pmu_pub 5 logics_pub 6 predictive_maintenance_pub 7 ganglia_pub 8 slurm_pub 9 nvidia_pub 10 weather_pub 11 dstat_pub 12 examon-ai_pub 13 nagios_pub In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(tag_key = 'plugin', tag_value='confluent_pub') \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(tag_key = 'plugin', tag_value='confluent_pub') \\     .execute()  display(df) name 0 12V 1 1U_Stg_HDD0_Pres 2 1U_Stg_HDD1_Pres 3 1U_Stg_HDD2_Pres 4 1U_Stg_HDD3_Pres ... ... 380 Vcpu2 381 Voltage_Fault 382 XCC_Corrupted 383 XCC_SWitchover 384 XCC_Switchover <p>385 rows \u00d7 1 columns</p> In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(tag_key = 'plugin', tag_value='confluent_pub') \\\n    .DESCRIBE(tag_key = 'cluster', tag_value='marconi') \\\n    .DESCRIBE(tag_key = 'part', tag_value='skylake') \\\n    .JOIN(how='inner') \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(tag_key = 'plugin', tag_value='confluent_pub') \\     .DESCRIBE(tag_key = 'cluster', tag_value='marconi') \\     .DESCRIBE(tag_key = 'part', tag_value='skylake') \\     .JOIN(how='inner') \\     .execute()  display(df) name 0 All_CPUs 1 All_DIMMs 2 All_PCI_Error 3 Ambient_Temp 4 Aux_Log ... ... 150 TPM_TCM_Lock 151 TXT_ACM_Module 152 XCC_Corrupted 153 XCC_SWitchover 154 XCC_Switchover <p>155 rows \u00d7 1 columns</p> In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(tag_key = 'plugin', tag_value='nagios_pub') \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(tag_key = 'plugin', tag_value='nagios_pub') \\     .execute()  display(df) name 0 hostscheduleddowtimecomments 1 plugin_output 2 state <p>Check the tags available for the 'plugin_output' metric</p> In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(metric='plugin_output') \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(metric='plugin_output') \\     .execute()  display(df) name tag key tag values 0 plugin_output chnl [data] 1 plugin_output cluster [galileo, marconi, marconi100] 2 plugin_output description [EFGW_cluster::status::availability, EFGW_clus... 3 plugin_output host_group [compute, compute,cincompute, containers, cumu... 4 plugin_output nagiosdrained [0, 1] 5 plugin_output node [aggregation-mgt, comlab01, deepops, dgx01, dg... 6 plugin_output org [cineca] 7 plugin_output plugin [nagios_pub] 8 plugin_output rack [201, 202, 205, 206, 207, 208, 209, 210, 211, ... 9 plugin_output slot [01, 02, 03, 04, 05, 06, 07, 08, 09, 1, 10, 11... 10 plugin_output state [0, 1, 2, 3] 11 plugin_output state_type [0, 1] <p>The 'description' tag may have some hints about the services monitored by this plugin. Lets check it:</p> In\u00a0[\u00a0]: Copied! <pre>df[df['tag key'] == 'description']['tag values'].values[0]\n</pre> df[df['tag key'] == 'description']['tag values'].values[0]  Out[\u00a0]: <pre>['EFGW_cluster::status::availability',\n 'EFGW_cluster::status::criticality',\n 'EFGW_cluster::status::internal',\n 'GALILEO_cluster::status::availability',\n 'GALILEO_cluster::status::criticality',\n 'GALILEO_cluster::status::internal',\n 'afs::blocked_conn::status',\n 'afs::bosserver::status',\n 'afs::ptserver::status',\n 'afs::space::status',\n 'afs::vlserver::status',\n 'alive::ping',\n 'backup::afs::status',\n 'backup::eufus_gw::status',\n 'backup::local::status',\n 'backup::masters::status',\n 'backup::shared::status',\n 'batchs::JobsH',\n 'batchs::client',\n 'batchs::client::serverrespond',\n 'batchs::client::state',\n 'batchs::manager',\n 'batchs::manager::state',\n 'bmc::events',\n 'cluster::status::availability',\n 'cluster::status::criticality',\n 'cluster::status::internal',\n 'cluster::status::wattage',\n 'cluster::us::availability',\n 'cluster::us::criticality',\n 'container::check::health',\n 'container::check::internal',\n 'container::check::mounts',\n 'core::total',\n 'crm::resources::m100',\n 'crm::status::m100',\n 'dev::ipmi::events',\n 'dev::raid::status',\n 'dev::swc::bntfru',\n 'dev::swc::bnthealth',\n 'dev::swc::bnttemp',\n 'dev::swc::confcheck',\n 'dev::swc::confcheckself',\n 'dev::swc::cumulushealth',\n 'dev::swc::cumulussensors',\n 'dev::swc::isl',\n 'dev::swc::isleth',\n 'dev::swc::mlxhealth',\n 'dev::swc::mlxsensors',\n 'file::integrity',\n 'filesys::dres::mount',\n 'filesys::eurofusion::mount',\n 'filesys::local::avail',\n 'filesys::local::mount',\n 'filesys::shared::mount',\n 'firewalld::status',\n 'galera::status::Integrity',\n 'galera::status::NodeStatus',\n 'galera::status::ReplicaStatus',\n 'globus::gridftp',\n 'globus::gsissh',\n 'gss::rg::encl',\n 'gss::rg::pdisks',\n 'gss::rg::peer',\n 'gss::rg::vdisks',\n 'memory::phys::total',\n 'monitoring::health',\n 'net::ib::status',\n 'net::opa',\n 'net::opa::edge_director_links_status',\n 'net::opa::edge_link_err_rate',\n 'net::opa::edge_link_quality',\n 'net::opa::edge_status',\n 'net::opa::pciwidth',\n 'nfs::rpc::status',\n 'nvidia::configuration',\n 'nvidia::memory::replace',\n 'nvidia::memory::retirement',\n 'service::MedeA',\n 'service::cert',\n 'service::galera',\n 'service::galera::mysql',\n 'service::galera:arbiter',\n 'service::galera:mysql',\n 'service::ganglia',\n 'service::nxserver',\n 'service::nxserver::sessions',\n 'service::unicore::tsi',\n 'service::unicore::uftpd',\n 'ssh::daemon',\n 'sys::arcldap::status',\n 'sys::corosync::rings',\n 'sys::cpus::freq',\n 'sys::glusterfs::dgx',\n 'sys::glusterfs::examon',\n 'sys::glusterfs::home',\n 'sys::glusterfs::install',\n 'sys::glusterfs::install8',\n 'sys::glusterfs::scratch',\n 'sys::glusterfs::secinv',\n 'sys::glusterfs::slurm',\n 'sys::glusterfs::slurmstate',\n 'sys::glusterfs::status',\n 'sys::gpfs::status',\n 'sys::ldap_srv::status',\n 'sys::orphaned_cgroups::count',\n 'sys::pacemaker::crm',\n 'sys::rvitals',\n 'sys::sssd::events',\n 'sys::xcatpod::sync',\n 'unicore::tsi',\n 'unicore::uftpd',\n 'vm::virsh::state']</pre> <p>Lets see if there are services in a 'critical' state (2) and which node affect:</p> In\u00a0[\u00a0]: Copied! <pre>data = sq.SELECT('node','cluster','description','state') \\\n    .FROM('plugin_output') \\\n    .WHERE(plugin='nagios_pub', state='2') \\\n    .TSTART(30, 'minutes') \\\n    .execute()\n\ndisplay(data.df_table.head(10))\n</pre> data = sq.SELECT('node','cluster','description','state') \\     .FROM('plugin_output') \\     .WHERE(plugin='nagios_pub', state='2') \\     .TSTART(30, 'minutes') \\     .execute()  display(data.df_table.head(10)) In\u00a0[\u00a0]: Copied! <pre>data.df_table.shape\n</pre> data.df_table.shape Out[\u00a0]: <pre>(8548, 7)</pre> In\u00a0[\u00a0]: Copied! <pre>data = sq.SELECT('cluster','part','node') \\\n    .FROM('Sys_Power') \\\n    .WHERE(cluster='marconi', part='skylake') \\\n    .TSTART(30, 'minutes') \\\n    .AGGRBY('avg', sampling_value=1, sampling_unit='minutes') \\\n    .execute()\n\ndisplay(data.df_table.head())\n</pre> data = sq.SELECT('cluster','part','node') \\     .FROM('Sys_Power') \\     .WHERE(cluster='marconi', part='skylake') \\     .TSTART(30, 'minutes') \\     .AGGRBY('avg', sampling_value=1, sampling_unit='minutes') \\     .execute()  display(data.df_table.head()) In\u00a0[\u00a0]: Copied! <pre>data.df_table.shape\n</pre> data.df_table.shape Out[\u00a0]: <pre>(46503, 6)</pre> <p>Check the number of nodes ('node' tag):</p> In\u00a0[\u00a0]: Copied! <pre>display(data.df_table.nunique())\n</pre> display(data.df_table.nunique()) <pre>timestamp      88\nvalue          37\nname            1\ncluster         1\npart            1\nnode         3121\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>data.to_series(flat_index=True, interp='time', dropna=True, columns=['node'])\n\ndisplay(data.df_ts.head())\n</pre> data.to_series(flat_index=True, interp='time', dropna=True, columns=['node'])  display(data.df_ts.head()) node r129c01s01 r129c01s02 r129c01s03 r129c01s04 r129c02s01 r129c02s02 r129c02s03 r129c02s04 r129c03s01 r129c03s02 ... r183c14s03 r183c14s04 r183c15s01 r183c15s02 r183c15s03 r183c15s04 r183c16s01 r183c16s02 r183c16s03 r183c16s04 timestamp 2023-09-22 17:49:00.061000+02:00 120.0 120.0 120.0 120.0 120.0 120.0 120.0 134.996730 282.488555 262.508175 ... 270.001292 259.998708 319.988375 309.997417 299.997417 329.989667 290.0 249.998708 260.005167 329.997417 2023-09-22 17:50:00.027000+02:00 120.0 120.0 120.0 120.0 120.0 120.0 120.0 130.000625 265.002187 274.998438 ... 272.499865 257.500135 297.501219 305.000271 295.000271 310.001083 290.0 247.500135 269.999458 325.000271 2023-09-22 17:50:00.031000+02:00 120.0 120.0 120.0 120.0 120.0 120.0 120.0 130.000292 265.001021 274.999271 ... 272.500031 257.499969 297.499719 304.999938 294.999938 309.999750 290.0 247.499969 270.000125 324.999938 2023-09-22 17:51:00.027000+02:00 120.0 120.0 120.0 120.0 120.0 120.0 120.0 125.001687 247.505905 287.495782 ... 274.999854 255.000146 275.001313 300.000292 290.000292 290.001167 290.0 245.000146 279.999417 320.000292 2023-09-22 17:51:00.031000+02:00 120.0 120.0 120.0 120.0 120.0 120.0 120.0 125.001354 247.504739 287.496615 ... 275.000021 254.999979 274.999813 299.999958 289.999958 289.999833 290.0 244.999979 280.000083 319.999958 <p>5 rows \u00d7 3121 columns</p> In\u00a0[\u00a0]: Copied! <pre>data.df_ts.mean().sum()\n</pre> data.df_ts.mean().sum() Out[\u00a0]: <pre>859025.39826763</pre> <p>First look for metrics with critical status. Use the intersection: search metrics having...</p> In\u00a0[\u00a0]: Copied! <pre>df = sq.DESCRIBE(tag_key = 'plugin', tag_value='confluent_pub') \\\n    .DESCRIBE(tag_key = 'health', tag_value='critical') \\\n    .DESCRIBE(tag_key = 'part', tag_value='skylake') \\\n    .JOIN() \\\n    .execute()\n\ndisplay(df)\n</pre> df = sq.DESCRIBE(tag_key = 'plugin', tag_value='confluent_pub') \\     .DESCRIBE(tag_key = 'health', tag_value='critical') \\     .DESCRIBE(tag_key = 'part', tag_value='skylake') \\     .JOIN() \\     .execute()  display(df) name 0 All_DIMMs 1 All_PCI_Error 2 Ambient_Temp 3 CMOS_Battery 4 CPU_1_DTS ... ... 61 PSU2_Failure 62 PSU2_IN_Failure 63 Power_Supply_1 64 Power_Supply_2 65 SysBrd_Vol_Fault <p>66 rows \u00d7 1 columns</p> <p>For example, lets check for CPU_1_Overtemp metric over the last year to find the affected nodes and the time period</p> In\u00a0[\u00a0]: Copied! <pre># show the tags to filter\ndf = sq.DESCRIBE(metric='CPU_1_Overtemp') \\\n    .execute()\n\ndisplay(df)\n</pre> # show the tags to filter df = sq.DESCRIBE(metric='CPU_1_Overtemp') \\     .execute()  display(df) name tag key tag values 0 CPU_1_Overtemp chnl [data] 1 CPU_1_Overtemp cluster [galileo, marconi] 2 CPU_1_Overtemp health [critical, failed, ok, warning] 3 CPU_1_Overtemp node [r054c02s01, r054c02s02, r054c02s03, r054c02s0... 4 CPU_1_Overtemp org [cineca] 5 CPU_1_Overtemp part [knl, skylake] 6 CPU_1_Overtemp plugin [confluent_pub] 7 CPU_1_Overtemp type [Temperature] In\u00a0[\u00a0]: Copied! <pre># query\ndata = sq.SELECT('*') \\\n    .FROM('CPU_1_Overtemp') \\\n    .WHERE(part='skylake', health='critical') \\\n    .TSTART(1,'years') \\\n    .execute()\n\ndisplay(data.df_table.head())\n</pre> # query data = sq.SELECT('*') \\     .FROM('CPU_1_Overtemp') \\     .WHERE(part='skylake', health='critical') \\     .TSTART(1,'years') \\     .execute()  display(data.df_table.head()) timestamp value name chnl cluster health node org part plugin type 0 2022-12-13 14:05:00.032000+01:00 critical CPU_1_Overtemp data marconi critical r135c11s01 cineca skylake confluent_pub Temperature 1 2023-09-14 11:53:00.038000+02:00 critical CPU_1_Overtemp data marconi critical r137c11s03 cineca skylake confluent_pub Temperature 2 2023-05-26 11:47:00.151000+02:00 critical CPU_1_Overtemp data marconi critical r138c02s02 cineca skylake confluent_pub Temperature 3 2023-05-26 23:47:00.034000+02:00 critical CPU_1_Overtemp data marconi critical r138c02s02 cineca skylake confluent_pub Temperature 4 2023-05-30 13:16:00.034000+02:00 critical CPU_1_Overtemp data marconi critical r138c02s02 cineca skylake confluent_pub Temperature <p>Show the first value of each node (when the anomaly appeared for the first time)</p> In\u00a0[\u00a0]: Copied! <pre>display(data \\\n        .df_table \\\n        .groupby('node') \\\n        .first() \\\n        .sort_values(by=['timestamp'],ascending=False) \\\n        .head())\n</pre> display(data \\         .df_table \\         .groupby('node') \\         .first() \\         .sort_values(by=['timestamp'],ascending=False) \\         .head()) timestamp value name chnl cluster health org part plugin type node r137c11s03 2023-09-14 11:53:00.038000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r143c04s03 2023-06-22 15:27:00.217000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r143c11s01 2023-06-13 19:40:00.031000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r138c13s02 2023-06-01 15:51:00.037000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r138c02s02 2023-05-26 11:47:00.151000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature <p>Show the last value of each node (when the anomaly was removed/solved)</p> In\u00a0[\u00a0]: Copied! <pre>display(data \\\n        .df_table \\\n        .groupby('node') \\\n        .last() \\\n        .sort_values(by=['timestamp'],ascending=False) \\\n        .head())\n</pre> display(data \\         .df_table \\         .groupby('node') \\         .last() \\         .sort_values(by=['timestamp'],ascending=False) \\         .head()) timestamp value name chnl cluster health org part plugin type node r138c13s02 2023-09-17 22:29:00.175000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r137c11s03 2023-09-14 11:53:00.038000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r143c02s04 2023-09-02 08:09:00.031000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r143c04s03 2023-06-22 15:47:00.162000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature r143c11s01 2023-06-14 06:02:00.027000+02:00 critical CPU_1_Overtemp data marconi critical cineca skylake confluent_pub Temperature <p>For example, node 'r145c10s04' showed a crtical status for the CPU1 temperature starting from 2019-09-23 16:54 to 2019-09-26 04:27. Lets check it plotting that range plus 1 hour before and after:</p> In\u00a0[\u00a0]: Copied! <pre>data = sq.SELECT('*') \\\n    .FROM('CPU_1_Temp') \\\n    .WHERE(node='r145c10s04') \\\n    .TSTART('23-09-2019 15:54:00') \\\n    .TSTOP('26-09-2019 05:27:00') \\\n    .execute()\n\ndata.to_series(flat_index=True, interp='time', dropna=True, columns=['node']).df_ts.plot(figsize=[15,12])\n</pre> data = sq.SELECT('*') \\     .FROM('CPU_1_Temp') \\     .WHERE(node='r145c10s04') \\     .TSTART('23-09-2019 15:54:00') \\     .TSTOP('26-09-2019 05:27:00') \\     .execute()  data.to_series(flat_index=True, interp='time', dropna=True, columns=['node']).df_ts.plot(figsize=[15,12]) Out[\u00a0]: <pre>&lt;Axes: xlabel='timestamp'&gt;</pre> <p>Where we can see values greater than 90 \u00b0C for the CPU1</p> In\u00a0[\u00a0]: Copied! <pre># Ask for all galileo jobs executed between '28-09-2019 08:09:00' and '30-09-2019 08:09:00'\n\nimport json\n\n# Setup\nsq.jc.JOB_TABLES.add('job_info_galileo')\n\ndata = sq.SELECT('*') \\\n    .FROM('job_info_galileo') \\\n    .TSTART('28-09-2019 08:09:00') \\\n    .TSTOP('30-09-2019 08:09:00') \\\n    .execute()\n\ndf = pd.DataFrame(json.loads(data))\ndf.head()\n</pre> # Ask for all galileo jobs executed between '28-09-2019 08:09:00' and '30-09-2019 08:09:00'  import json  # Setup sq.jc.JOB_TABLES.add('job_info_galileo')  data = sq.SELECT('*') \\     .FROM('job_info_galileo') \\     .TSTART('28-09-2019 08:09:00') \\     .TSTOP('30-09-2019 08:09:00') \\     .execute()  df = pd.DataFrame(json.loads(data)) df.head() In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape In\u00a0[\u00a0]: Copied! <pre># Ask for all galileo jobs executed between '28-09-2019 08:09:00' and '30-09-2019 08:09:00',\n# allocated on node \"r038c04s03\"\n\ndata = sq.SELECT('*') \\\n    .FROM('job_info_galileo') \\\n    .WHERE(node='r038c04s03') \\\n    .TSTART('28-09-2019 08:09:00') \\\n    .TSTOP('30-09-2019 08:09:00') \\\n    .execute()\n\ndf = pd.DataFrame(json.loads(data))\ndf.head()\n</pre> # Ask for all galileo jobs executed between '28-09-2019 08:09:00' and '30-09-2019 08:09:00', # allocated on node \"r038c04s03\"  data = sq.SELECT('*') \\     .FROM('job_info_galileo') \\     .WHERE(node='r038c04s03') \\     .TSTART('28-09-2019 08:09:00') \\     .TSTOP('30-09-2019 08:09:00') \\     .execute()  df = pd.DataFrame(json.loads(data)) df.head() In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape In\u00a0[\u00a0]: Copied! <pre># Ask for all galileo jobs executed between '28-09-2019 08:09:00' and '30-09-2019 08:09:00',\n# allocated on node \"r038c04s03\" and job_state = 'FAILED'\n\ndata = sq.SELECT('*') \\\n    .FROM('job_info_galileo') \\\n    .WHERE(node='r038c04s03', job_state='FAILED') \\\n    .TSTART('28-09-2019 08:09:00') \\\n    .TSTOP('30-09-2019 08:09:00') \\\n    .execute()\n\ndf = pd.DataFrame(json.loads(data))\ndf.head()\n</pre> # Ask for all galileo jobs executed between '28-09-2019 08:09:00' and '30-09-2019 08:09:00', # allocated on node \"r038c04s03\" and job_state = 'FAILED'  data = sq.SELECT('*') \\     .FROM('job_info_galileo') \\     .WHERE(node='r038c04s03', job_state='FAILED') \\     .TSTART('28-09-2019 08:09:00') \\     .TSTOP('30-09-2019 08:09:00') \\     .execute()  df = pd.DataFrame(json.loads(data)) df.head() In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape In\u00a0[\u00a0]: Copied! <pre># Marconi100 jobs\n\n# Setup for Marconi100\nsq.jc.JOB_TABLES.add('job_info_marconi100')\n\ndata = sq.SELECT('*') \\\n    .FROM('job_info_marconi100') \\\n    .TSTART('28-09-2020 08:09:00') \\\n    .TSTOP('30-09-2020 08:09:00') \\\n    .execute()\n\ndf = pd.read_json(data)\ndf.head()\n</pre> # Marconi100 jobs  # Setup for Marconi100 sq.jc.JOB_TABLES.add('job_info_marconi100')  data = sq.SELECT('*') \\     .FROM('job_info_marconi100') \\     .TSTART('28-09-2020 08:09:00') \\     .TSTOP('30-09-2020 08:09:00') \\     .execute()  df = pd.read_json(data) df.head() In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape Out[\u00a0]: <pre>(11614, 110)</pre> In\u00a0[\u00a0]: Copied! <pre>import time\n\n# One month of data\ntstart = '01-04-2021 00:00:00'\ntstop = '30-04-2021 00:00:00'\n\nt0 = time.time()\ndata = sq.SELECT('*') \\\n    .FROM('job_info_marconi100') \\\n    .TSTART(tstart) \\\n    .TSTOP(tstop) \\\n    .execute_async()\n\nprint('Elapsed Time: %f seconds' % (time.time() - t0))\n\ndf = pd.read_json(data)\ndf.head()\n</pre> import time  # One month of data tstart = '01-04-2021 00:00:00' tstop = '30-04-2021 00:00:00'  t0 = time.time() data = sq.SELECT('*') \\     .FROM('job_info_marconi100') \\     .TSTART(tstart) \\     .TSTOP(tstop) \\     .execute_async()  print('Elapsed Time: %f seconds' % (time.time() - t0))  df = pd.read_json(data) df.head() In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape Out[\u00a0]: <pre>(109608, 110)</pre>"},{"location":"Users/Demo_ExamonQL/#exploring-the-examon-database","title":"Exploring the ExaMon database\u00b6","text":"<p>This introductory notebook is a tutorial that will help you take your first steps with the <code>examon-client</code>, a tool that allows you to interact with the ExaMon database.</p> <p>The tutorial will show you the basics of how to obtain information about the data stored in the database (metadata) and how to make real queries and obtain a dataframe as a result.</p> <p>The tutorial will use the ExaMon instance running at CINECA as an example and also the notebook execution can take place directly on Google Colab. In this case it is recommended to mount your Drive account. Alternatively, you can download the notebook (top right button) and run it locally.</p> <p>If you are interested in working on the CINECA ExaMon instance, to obtain the ExaMon credentials please contact:</p> <ul> <li>Andrea Bartolini</li> <li>Francesco Beneventi</li> </ul> <p>Please note: by using your ExaMon account you are able to access data owned by CINECA and are therefore subject to the same privacy regulations that every CINECA user is required to follow.</p>"},{"location":"Users/Demo_ExamonQL/#examon-setup","title":"Examon setup\u00b6","text":""},{"location":"Users/Demo_ExamonQL/#metric-list","title":"Metric list\u00b6","text":"<p>To start with Examon, it is recommended that you first get a list of the sensors contained in the database. The initial object (ExamonQL) instantiation will do a full db scan checking for all the metrics tags. This will happen only the first time since the client uses caches where possible to save the database bandwith.</p>"},{"location":"Users/Demo_ExamonQL/#tag-keys","title":"Tag Keys\u00b6","text":"<p>Each metric in the database comes with a set of tags (key;value) useful for filtering during queries. It is possible to obtain from the database all the possible tags (keys) associated to a specific metric.</p>"},{"location":"Users/Demo_ExamonQL/#tag-values","title":"Tag values\u00b6","text":"<p>It is possible to obtain all the possible values of all the tag keys of a given metric:</p>"},{"location":"Users/Demo_ExamonQL/#all-the-possible-values-of-a-given-tag-key","title":"All the possible values of a given tag key\u00b6","text":"<p>In this example we will search all the plugin names currently available in the Examon database.</p>"},{"location":"Users/Demo_ExamonQL/#metrics-having-a-given-tag-value","title":"Metrics having a given tag value\u00b6","text":"<p>Assume that we need to know the list of the metrics having a given tag (key, value). In this example, we get the list of all metrics inserted into the db by the 'confluent_pub' examon plugin.</p>"},{"location":"Users/Demo_ExamonQL/#metrics-valid-only-for-marconi-skaylake-nodes","title":"Metrics valid only for Marconi skaylake nodes\u00b6","text":"<p>Some metrics are valid (exist) only for a subset of the monitored resources. In this example we will search for the metrics collected by the 'confluent_pub' plugin and for the 'marconi' cluster and for only the 'skylake' partition. The 'JOIN' command let you 'intersect' ('inner' join) the results of each DESCRIBE command.</p>"},{"location":"Users/Demo_ExamonQL/#metrics-collected-by-the-nagios_pub-plugin","title":"Metrics collected by the 'nagios_pub' plugin\u00b6","text":""},{"location":"Users/Demo_ExamonQL/#query-examples","title":"Query Examples\u00b6","text":""},{"location":"Users/Demo_ExamonQL/#1-marconi-skylake-power-consumption","title":"1) Marconi Skylake Power Consumption\u00b6","text":""},{"location":"Users/Demo_ExamonQL/#time-series-format","title":"Time Series Format\u00b6","text":"<p>Reshape the 'df_table' to a time series table: first column (index) = timestamp, remaining columns = nodes power vectors.</p>"},{"location":"Users/Demo_ExamonQL/#skylake-partition-total-power-consuption","title":"Skylake partition total power consuption\u00b6","text":"<p>Total average power in the previous 30 minutes</p>"},{"location":"Users/Demo_ExamonQL/#2-looking-for-failures","title":"2) Looking for failures\u00b6","text":""},{"location":"Users/Demo_ExamonQL/#job-scheduler-data","title":"Job scheduler data\u00b6","text":"NOTE This is an experimental feature and is subject to change in future versions.  <p>Currently the job scheduler data is collected as per-job data in plain Cassandra tables. The available tables in the database are</p> <ul> <li>job_info_galileo: Galileo jobs data</li> <li>job_info_marconi: Marconi jobs data</li> </ul> <p>This is a description of the data currently stored (where available) for each executed job:</p> Table fields Description account charge to specified account accrue_time time job is eligible for running admin_comment administrator's arbitrary comment alloc_node local node and system id making the resource allocation alloc_sid local sid making resource alloc array_job_id job_id of a job array or 0 if N/A array_max_tasks Maximum number of running tasks array_task_id task_id of a job array array_task_str string expression of task IDs in this record assoc_id association id for job batch_features features required for batch script's node batch_flag 1 if batch: queued job with script batch_host name of host running batch script billable_tres billable TRES cache. updated upon resize bitflags Various job flags boards_per_node boards per node required by job burst_buffer burst buffer specifications burst_buffer_state burst buffer state info command command to be executed, built from submitted  job's argv and NULL for salloc command comment arbitrary comment contiguous 1 if job requires contiguous nodes core_spec specialized core count cores_per_socket cores per socket required by job cpu_freq_gov cpu frequency governor cpu_freq_max Maximum cpu frequency cpu_freq_min Minimum cpu frequency cpus_alloc_layout map: list of cpu allocated per node cpus_allocated map: number of cpu allocated per node cpus_per_task number of processors required for each task cpus_per_tres semicolon delimited list of TRES=# values dependency synchronize job execution with other jobs derived_ec highest exit code of all job steps eligible_time time job is eligible for running end_time time of termination, actual or expected exc_nodes comma separated list of excluded nodes exit_code exit code for job (status from wait call) features comma separated list of required features group_id group job submitted as job_id job ID job_state state of the job, see enum job_states last_sched_eval last time job was evaluated for scheduling licenses licenses required by the job max_cpus maximum number of cpus usable by job max_nodes maximum number of nodes usable by job mem_per_cpu boolean mem_per_node boolean mem_per_tres semicolon delimited list of TRES=# values min_memory_cpu minimum real memory required per allocated CPU min_memory_node minimum real memory required per node name name of the job network network specification nice requested priority change nodes list of nodes allocated to job ntasks_per_board number of tasks to invoke on each board ntasks_per_core number of tasks to invoke on each core ntasks_per_core_str number of tasks to invoke on each core  as string ntasks_per_node number of tasks to invoke on each node ntasks_per_socket number of tasks to invoke on each socket ntasks_per_socket_str number of tasks to invoke on each socket as string num_cpus minimum number of cpus required by job num_nodes minimum number of nodes required by job partition name of assigned partition pn_min_cpus minimum # CPUs per node, default=0 pn_min_memory minimum real memory per node, default=0 pn_min_tmp_disk minimum tmp disk per node, default=0 power_flags power management flags,  see SLURM_POWER_FLAGS_ pre_sus_time time job ran prior to last suspend preempt_time preemption signal time priority relative priority of the job, 0=held, 1=required nodes DOWN/DRAINED profile Level of acct_gather_profile {all / none} qos Quality of Service reboot node reboot requested before start req_nodes comma separated list of required nodes req_switch Minimum number of switches requeue enable or disable job requeue option resize_time time of latest size change restart_cnt count of job restarts resv_name reservation name run_time job run time (seconds) run_time_str job run time (seconds) as string sched_nodes list of nodes scheduled to be used for job shared 1 if job can share nodes with other jobs show_flags conveys level of details requested sockets_per_board sockets per board required by job sockets_per_node sockets per node required by job start_time time execution begins, actual or expected state_reason reason job still pending or failed, see slurm.h:enum job_state_reason std_err pathname of job's stderr file std_in pathname of job's stdin file std_out pathname of job's stdout file submit_time time of job submission suspend_time time job last suspended or resumed system_comment slurmctld's arbitrary comment threads_per_core threads per core required by job time_limit maximum run time in minutes or INFINITE time_limit_str maximum run time in minutes or INFINITE as string time_min minimum run time in minutes or INFINITE tres_alloc_str tres used in the job as string tres_bind Task to TRES binding directives tres_freq TRES frequency directives tres_per_job semicolon delimited list of TRES=# values tres_per_node semicolon delimited list of TRES=# values tres_per_socket semicolon delimited list of TRES=# values tres_per_task semicolon delimited list of TRES=# values tres_req_str tres reqeusted in the job as string user_id user the job runs as wait4switch Maximum time to wait for minimum switches wckey wckey for job work_dir pathname of working directory"},{"location":"Users/Demo_ExamonQL/#query-examples","title":"Query examples\u00b6","text":"<p>Queries can be executed as usual but paying attention to the following limitations:</p> <ul> <li>both TSTART and TSTOP statements must be specified</li> <li>the date currently is supported only in the string format</li> <li>pushdown filters (executed on the datastore) are available only for a subset of table columns:</li> <li>job_id</li> <li>job_state</li> <li>account</li> <li>user_id</li> <li>node (keys of cpus_alloc_layout table column)</li> </ul>"},{"location":"Users/Demo_ExamonQL/#asynchronous-queries","title":"Asynchronous queries\u00b6","text":"<p>In case of big queries it can be useful to use the asynchronous mode (available from client version v0.4.0)</p>"},{"location":"Users/Getting_started/","title":"Getting Started","text":"<ul> <li>Introductory notebook</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/08/31/first-post/","title":"First Post","text":"<p>Hello World!</p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/test/","title":"Test","text":""}]}